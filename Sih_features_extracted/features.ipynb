{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Pywavelets\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-macosx_11_0_arm64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/inviforce/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from Pywavelets) (1.24.3)\n",
      "Installing collected packages: Pywavelets\n",
      "Successfully installed Pywavelets-1.4.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/inviforce/.pyenv/versions/3.8.12/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Pywavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyunicorn\n",
      "  Downloading pyunicorn-0.8.0.tar.gz (31.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.3 MB 703 kB/s eta 0:00:01     |██████████▊                     | 10.5 MB 474 kB/s eta 0:00:44     |██████████████████████████████▋ | 29.9 MB 658 kB/s eta 0:00:03\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.10 in /Users/inviforce/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from pyunicorn) (1.10.1)\n",
      "Collecting igraph>=0.11\n",
      "  Downloading igraph-0.11.8-cp38-cp38-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 259 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.66\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5netcdf==1.1.*\n",
      "  Downloading h5netcdf-1.1.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in /Users/inviforce/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from pyunicorn) (1.24.3)\n",
      "Requirement already satisfied: packaging in /Users/inviforce/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from h5netcdf==1.1.*->pyunicorn) (24.2)\n",
      "Requirement already satisfied: h5py in /Users/inviforce/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from h5netcdf==1.1.*->pyunicorn) (3.11.0)\n",
      "Collecting texttable>=1.6.2\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: pyunicorn\n",
      "  Building wheel for pyunicorn (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyunicorn: filename=pyunicorn-0.8.0-cp38-cp38-macosx_14_0_arm64.whl size=572090 sha256=f2e8b374fbbc5f73bc19f07331433d74fd4b3cd122371c5d5e721d191ec72558\n",
      "  Stored in directory: /Users/inviforce/Library/Caches/pip/wheels/3c/2b/31/857a9b3cf7a0b6ee5a63bfe414a536d1613bb49779dba9fc6b\n",
      "Successfully built pyunicorn\n",
      "Installing collected packages: texttable, tqdm, igraph, h5netcdf, pyunicorn\n",
      "Successfully installed h5netcdf-1.1.0 igraph-0.11.8 pyunicorn-0.8.0 texttable-1.7.0 tqdm-4.67.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/inviforce/.pyenv/versions/3.8.12/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyunicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nolds\n",
      "  Downloading nolds-0.6.1-py2.py3-none-any.whl (225 kB)\n",
      "\u001b[K     |████████████████████████████████| 225 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[K     |████████████████████████████████| 491 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/inviforce/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from nolds) (56.0.0)\n",
      "Requirement already satisfied: numpy<3.0,>1.0 in /Users/inviforce/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from nolds) (1.24.3)\n",
      "Installing collected packages: future, nolds\n",
      "Successfully installed future-1.0.0 nolds-0.6.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/inviforce/.pyenv/versions/3.8.12/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_str(k):\n",
    "    binary_string_final = \"\"\n",
    "    for hex_string in k:\n",
    "        pairs = [hex_string[i:i+2] for i in range(0, len(hex_string), 2)]\n",
    "        binary_list = [bin(int(pair, 16))[2:].zfill(8) for pair in pairs]\n",
    "        binary_string = ''.join(binary_list)\n",
    "        binary_string_final += binary_string\n",
    "    return binary_string_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_analysis(k):\n",
    "  #this function will be used to measure the randomness of cipher text this feature\n",
    "  # might be a little less useful\n",
    "  prob_0 = k.count('0') / len(k)exit\n",
    "  \n",
    "  prob_1 = k.count('1') / len(k)\n",
    "\n",
    "    # Avoid log(0) by ensuring probabilities > 0\n",
    "  entropy = 0\n",
    "  if prob_0 > 0:\n",
    "      entropy -= prob_0 * np.log2(prob_0)\n",
    "  if prob_1 > 0:\n",
    "      entropy -= prob_1 * np.log2(prob_1)\n",
    "\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to generate a random binary string of given length\n",
    "def generate_random_binary_string(length):\n",
    "    return ''.join(random.choice(['0', '1']) for _ in range(length))\n",
    "\n",
    "# Generate 10 random binary strings of varying lengths between 400 and 600\n",
    "random_binary_strings = [generate_random_binary_string(random.randint(400, 600)) for _ in range(20)]\n",
    "\n",
    "# Display the generated binary strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_binary_strings.sort(key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.99989825090143\n",
      "466 0.9998804121079263\n",
      "471 0.9999967483555232\n",
      "476 0.9943766625699825\n",
      "482 0.9984966963146396\n",
      "513 0.9993831859860329\n",
      "518 0.9991288007981276\n",
      "525 0.9999345706643479\n",
      "527 0.9999350663467852\n",
      "529 0.9994199386491505\n",
      "536 0.9970955465743891\n",
      "536 0.9996384122227047\n",
      "537 0.9992769532498011\n",
      "539 0.9945082036445804\n",
      "540 0.9980597045705908\n",
      "546 0.9993804713475465\n",
      "560 0.9990797181805819\n",
      "563 0.9999977242320235\n",
      "584 0.9996954126191551\n",
      "599 0.9992741097724539\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),entropy_analysis(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_test(k):\n",
    "  #checks the deviation form stability\n",
    "    E_i = 0.5\n",
    "    prob_0 = k.count('0') / len(k)\n",
    "    prob_1 = k.count('1') / len(k)\n",
    "    chi_squared = (((prob_0 - E_i) ** 2) / E_i) + (((prob_1 - E_i) ** 2) / E_i)\n",
    "    deviation = chi_squared ** 0.5  # Taking the square root for stability test\n",
    "\n",
    "    return deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.01187648456057011\n",
      "466 0.012875536480686733\n",
      "471 0.002123142250530785\n",
      "476 0.08823529411764702\n",
      "482 0.04564315352697096\n",
      "513 0.029239766081871343\n",
      "518 0.034749034749034735\n",
      "525 0.00952380952380949\n",
      "527 0.009487666034155628\n",
      "529 0.02835538752362954\n",
      "536 0.06343283582089554\n",
      "536 0.02238805970149249\n",
      "537 0.031657355679702015\n",
      "539 0.08719851576994436\n",
      "540 0.051851851851851816\n",
      "546 0.029304029304029255\n",
      "560 0.035714285714285754\n",
      "563 0.001776198934280604\n",
      "584 0.02054794520547948\n",
      "599 0.031719532554257135\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),frequency_test(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length(k):\n",
    "  #used to find the A run of length k\n",
    "  #consists of exactly k identical bits and\n",
    "  #is bounded before and after with a bit of opposite value.\n",
    "  runs = [(key, len(list(group))) for key, group in groupby(k)]\n",
    "\n",
    "    # Separate runs of 0s and 1s\n",
    "  run_lengths_0 = [length for bit, length in runs if bit == '0']\n",
    "  run_lengths_1 = [length for bit, length in runs if bit == '1']\n",
    "\n",
    "    # Calculate mean run lengths, handle empty cases gracefully\n",
    "  mean_run_0 = np.mean(run_lengths_0) if run_lengths_0 else 0\n",
    "  mean_run_1 = np.mean(run_lengths_1) if run_lengths_1 else 0\n",
    "\n",
    "  return [mean_run_0, mean_run_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 [1.8571428571428572, 1.9017857142857142]\n",
      "466 [1.84375, 1.811023622047244]\n",
      "471 [2.0344827586206895, 2.0258620689655173]\n",
      "476 [2.4903846153846154, 2.0865384615384617]\n",
      "482 [1.9090909090909092, 1.7557251908396947]\n",
      "513 [2.1639344262295084, 2.024390243902439]\n",
      "518 [2.144, 2.0]\n",
      "525 [2.0703125, 2.03125]\n",
      "527 [2.094488188976378, 2.0390625]\n",
      "529 [2.0078125, 2.108527131782946]\n",
      "536 [2.2093023255813953, 1.9307692307692308]\n",
      "536 [1.955223880597015, 2.044776119402985]\n",
      "537 [2.036764705882353, 1.9259259259259258]\n",
      "539 [1.8222222222222222, 2.1703703703703705]\n",
      "540 [2.21875, 2.0]\n",
      "546 [1.8859060402684564, 1.7905405405405406]\n",
      "560 [2.027972027972028, 1.8881118881118881]\n",
      "563 [1.8733333333333333, 1.8926174496644295]\n",
      "584 [1.881578947368421, 1.9477124183006536]\n",
      "599 [1.9863013698630136, 2.1164383561643834]\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),run_length(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_weight(k):\n",
    "  #counts number of 1\n",
    "  return k.count('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 213\n",
      "466 230\n",
      "471 235\n",
      "476 217\n",
      "482 230\n",
      "513 249\n",
      "518 250\n",
      "525 260\n",
      "527 261\n",
      "529 272\n",
      "536 251\n",
      "536 274\n",
      "537 260\n",
      "539 293\n",
      "540 256\n",
      "546 265\n",
      "560 270\n",
      "563 282\n",
      "584 298\n",
      "599 309\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),hamming_weight(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_transition(segment):\n",
    "    \"\"\"\n",
    "    Count the number of bit transitions (changes from 0 to 1 or 1 to 0) in the binary string.\n",
    "\n",
    "    Parameters:\n",
    "        segment (str): Binary string.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of transitions.\n",
    "    \"\"\"\n",
    "    # Convert the binary string to a list of integers\n",
    "    data = list(map(int, segment))\n",
    "\n",
    "    # Initialize count of transitions\n",
    "    count = 0\n",
    "\n",
    "    # Loop through adjacent bits\n",
    "    for i in range(len(data) - 1):\n",
    "        count += abs(data[i] - data[i + 1])  # Compute transition\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 223\n",
      "466 254\n",
      "471 231\n",
      "476 207\n",
      "482 262\n",
      "513 244\n",
      "518 249\n",
      "525 255\n",
      "527 254\n",
      "529 256\n",
      "536 258\n",
      "536 267\n",
      "537 270\n",
      "539 269\n",
      "540 255\n",
      "546 296\n",
      "560 285\n",
      "563 298\n",
      "584 304\n",
      "599 291\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),bit_transition(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_skewness_kurtosis(k):\n",
    "    # Convert binary string into a list of integers\n",
    "    data = list(map(int, k))\n",
    "    n = len(data)\n",
    "    # Calculate Mean\n",
    "    mean = sum(data) / n\n",
    "    # Calculate Median\n",
    "    sorted_data = sorted(data)\n",
    "    if n % 2 == 1:  # Odd number of elements\n",
    "        median = sorted_data[n // 2]\n",
    "    else:  # Even number of elements\n",
    "        median = (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2\n",
    "    # Calculate Standard Deviation\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    std_dev = variance ** 0.5\n",
    "    if std_dev == 0:  # Handle edge case where all elements are identical\n",
    "        return 0,0\n",
    "    # Calculate Skewness\n",
    "    skewness = 3 * (mean - median) / std_dev\n",
    "    # Calculate Kurtosis\n",
    "    kurtosis = sum((x - mean) ** 4 for x in data) / (n * std_dev ** 4) - 3\n",
    "\n",
    "    return skewness, kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 (-2.9645796319826263, -1.9994357168652783)\n",
      "466 (2.96161888849291, -1.9993367722918178)\n",
      "471 (2.9936373205149014, -1.999981968986666)\n",
      "476 (2.7460044684123397, -1.9686137750653971)\n",
      "482 (2.8660575211055566, -1.9916494133885472)\n",
      "513 (2.913526448075665, -1.9965772179627386)\n",
      "518 (2.8975027848234562, -1.99516417910449)\n",
      "525 (2.971563339261904, -1.9996371552975147)\n",
      "527 (2.9716707535777287, -1.9996399043585897)\n",
      "529 (-2.916106388763656, -1.996781300068665)\n",
      "536 (2.815371341310719, -1.9838400782833532)\n",
      "536 (-2.933571102949169, -1.9979940937204017)\n",
      "537 (2.9064847203456803, -1.9959872257706133)\n",
      "539 (-2.7488750413894794, -1.9693526457449042)\n",
      "540 (2.8482759796652517, -1.9892165492957632)\n",
      "546 (2.9133390609377607, -1.9965621432888132)\n",
      "560 (2.8947038440620534, -1.9948914431672937)\n",
      "563 (-2.994676127126771, -1.9999873804295698)\n",
      "584 (-2.9389766743915064, -1.9983104144178154)\n",
      "599 (-2.9063038281892544, -1.9959714317598607)\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),binary_skewness_kurtosis(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(binary_seq, lag):\n",
    "    # Convert binary string into a list of integers\n",
    "    data = list(map(int, binary_seq))\n",
    "    n = len(data)\n",
    "    if lag >= n:\n",
    "        return \"Lag is too large for the sequence length.\"\n",
    "    # Calculate Mean\n",
    "    mean = sum(data) / n\n",
    "    # Calculate Variance\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    if variance == 0:  # Handle edge case where all elements are identical\n",
    "        return 0\n",
    "    # Calculate Autocorrelation for the given lag\n",
    "    autocorr = sum((data[t] - mean) * (data[t + lag] - mean) for t in range(n - lag)) / variance\n",
    "\n",
    "    return autocorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 -37.08893102202986\n",
      "466 -12.002652910832696\n",
      "471 -17.002217814641003\n",
      "476 -7.797733217088188\n",
      "482 19.11863354037287\n",
      "513 -7.56544359255206\n",
      "518 -12.715164179104416\n",
      "525 10.972060957910132\n",
      "527 -21.087647279117295\n",
      "529 22.64637216754396\n",
      "536 -22.517145453274647\n",
      "536 55.84688248732394\n",
      "537 8.593057484032215\n",
      "539 29.268195565914226\n",
      "540 -19.411091549295794\n",
      "546 -24.434674007923157\n",
      "560 -8.730523627075495\n",
      "563 0.9946619217082043\n",
      "584 13.798657718120593\n",
      "599 -29.509507867425476\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),autocorrelation(k,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_convolution(ciphertext, kernel=[1, -1]):\n",
    "    \"\"\"\n",
    "    Compute the convolution of a ciphertext with a given kernel and return the mean of the result.\n",
    "\n",
    "    Parameters:\n",
    "        ciphertext (list or str): Numeric representation of the ciphertext (can be binary string).\n",
    "        kernel (list or np.array): Convolutional filter or kernel.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean of the convolution result.\n",
    "    \"\"\"\n",
    "    # If the ciphertext is a string, convert it to a list of integers\n",
    "    if isinstance(ciphertext, str):\n",
    "        ciphertext = [int(bit) for bit in ciphertext]\n",
    "    \n",
    "    # Convert ciphertext and kernel to numpy arrays for efficient computation\n",
    "    ciphertext = np.array(ciphertext)\n",
    "    kernel = np.array(kernel)\n",
    "    \n",
    "    # Lengths of ciphertext and kernel\n",
    "    ciphertext_len = len(ciphertext)\n",
    "    kernel_len = len(kernel)\n",
    "    \n",
    "    # Length of the output\n",
    "    output_len = ciphertext_len - kernel_len + 1\n",
    "    \n",
    "    # Initialize the convolution result\n",
    "    convolution_result = np.zeros(output_len)\n",
    "    \n",
    "    # Perform the convolution\n",
    "    for i in range(output_len):\n",
    "        convolution_result[i] = np.sum(ciphertext[i:i+kernel_len] * kernel)\n",
    "    \n",
    "    # Return the mean of the convolution result\n",
    "    return np.mean(convolution_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.002380952380952381\n",
      "466 0.0\n",
      "471 0.002127659574468085\n",
      "476 0.002105263157894737\n",
      "482 0.0\n",
      "513 0.0\n",
      "518 -0.0019342359767891683\n",
      "525 0.0019083969465648854\n",
      "527 0.0\n",
      "529 0.0\n",
      "536 0.0\n",
      "536 0.001869158878504673\n",
      "537 0.0\n",
      "539 -0.0018587360594795538\n",
      "540 0.0018552875695732839\n",
      "546 0.0\n",
      "560 0.0017889087656529517\n",
      "563 0.0\n",
      "584 0.0\n",
      "599 0.0016722408026755853\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),compute_convolution(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_features_for_row(row, segment_sizes):\n",
    "    \"\"\"\n",
    "    Extract features for a single row of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataset containing binary ciphertext in column 4.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of features extracted for the row.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    binary_string = row[4]  # Column containing binary ciphertext\n",
    "\n",
    "    # Extract features for each segment size\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]  # Binary segment (first `size` bits)\n",
    "\n",
    "        # Pad with zeros if the segment is smaller than the required size\n",
    "        segment = segment.ljust(size, '0')\n",
    "\n",
    "        # Extract features for the segment\n",
    "        features[f\"entropy_{size}\"] = entropy_analysis(segment)\n",
    "        features[f\"frequency_deviation_{size}\"] = frequency_test(segment)\n",
    "        features[f\"mean_run_0_{size}\"], features[f\"mean_run_1_{size}\"] = run_length(segment)\n",
    "        features[f\"hamming_weight_{size}\"] = hamming_weight(segment)\n",
    "        features[f\"bit_transitions_{size}\"] = bit_transition(segment)\n",
    "        features[f\"skewness_{size}\"], features[f\"kurtosis_{size}\"] = binary_skewness_kurtosis(segment)\n",
    "        features[f\"autocorrelation_{size}\"] = autocorrelation(segment, lag=4)\n",
    "        features[f\"fractal_dimension_{size}\"] = fractal_dimension(segment)\n",
    "        \n",
    "        convolution_result = compute_convolution([int(bit) for bit in segment])\n",
    "        features[f\"convolution_{size}\"] = sum(convolution_result)\n",
    "        features[f\"complexity_{size}\"] = compute_ciphertext_complexity(segment)\n",
    "\n",
    "    return features\n",
    "\n",
    "def append_features_to_dataset(dataset, segment_sizes=[128,256,512,1024,2048,4096], n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Append features to the dataset for each ciphertext in column 4 using parallelization.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        n_jobs (int): Number of parallel jobs (-1 means using all available processors).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with appended feature columns.\n",
    "    \"\"\"\n",
    "    # Extract features in parallel for each row\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes) for _, row in dataset.iterrows()\n",
    "    )\n",
    "\n",
    "    # Combine the features back into the DataFrame\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def chi_square_test(data):\n",
    "    \"\"\"\n",
    "    Perform a chi-square test against a uniform distribution for binary data.\n",
    "\n",
    "    Parameters:\n",
    "        data (str or bytes): Binary string or bytes-like object.\n",
    "\n",
    "    Returns:\n",
    "        float: Chi-square statistic.\n",
    "    \"\"\"\n",
    "    # If the data is a string, convert it to a list of integers\n",
    "    if isinstance(data, str):\n",
    "        data = [int(bit) for bit in data]\n",
    "    \n",
    "    # Convert to numpy array for efficient computation\n",
    "    data = np.array(data, dtype=np.uint8)\n",
    "    \n",
    "    # Count occurrences of 0 and 1\n",
    "    observed = np.bincount(data, minlength=2)\n",
    "    \n",
    "    # Compute the expected frequency for uniform distribution\n",
    "    expected = np.full(2, len(data) / 2)\n",
    "    \n",
    "    # Compute the chi-square statistic\n",
    "    chi_square = np.sum((observed - expected) ** 2 / expected)\n",
    "    \n",
    "    return chi_square\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.05938242280285035\n",
      "466 0.07725321888412018\n",
      "471 0.0021231422505307855\n",
      "476 3.7058823529411766\n",
      "482 1.004149377593361\n",
      "513 0.43859649122807015\n",
      "518 0.6254826254826255\n",
      "525 0.047619047619047616\n",
      "527 0.04743833017077799\n",
      "529 0.42533081285444235\n",
      "536 2.156716417910448\n",
      "536 0.26865671641791045\n",
      "537 0.5381750465549349\n",
      "539 4.098330241187384\n",
      "540 1.451851851851852\n",
      "546 0.46886446886446886\n",
      "560 0.7142857142857143\n",
      "563 0.0017761989342806395\n",
      "584 0.2465753424657534\n",
      "599 0.6026711185308848\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),chi_square_test(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chi_square_test(data):\n",
    "    \"\"\"Perform chi-square test against uniform distribution.\"\"\"\n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    observed = np.bincount(np.frombuffer(data.encode('utf-8'), dtype=np.uint8), minlength=256)\n",
    "    expected = np.full(256, len(data) / 256)\n",
    "    chi_square = np.sum((observed - expected) ** 2 / expected)\n",
    "    return chi_square\n",
    "\n",
    "def extract_features_for_row(row, segment_sizes):\n",
    "    \"\"\"\n",
    "    Extract features for a single row of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataset containing binary ciphertext in column 4.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of features extracted for the row.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    binary_string = row[4]  # Column containing binary ciphertext\n",
    "\n",
    "    # Extract features for each segment size\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]  # Binary segment (first `size` bits)\n",
    "\n",
    "        # Pad with zeros if the segment is smaller than the required size\n",
    "        segment = segment.ljust(size, '0')\n",
    "\n",
    "        # Extract features for the segment\n",
    "        features[f\"chi_square_{size}\"] = chi_square_test(segment)\n",
    "\n",
    "    return features\n",
    "\n",
    "def append_features_to_dataset(dataset, segment_sizes=[128, 256, 512, 1024, 2048, 4096], n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Append features to the dataset for each ciphertext in column 4 using parallelization.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        n_jobs (int): Number of parallel jobs (-1 means using all available processors).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with appended feature columns.\n",
    "    \"\"\"\n",
    "    # Extract features in parallel for each row\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes) for _, row in dataset.iterrows()\n",
    "    )\n",
    "\n",
    "    # Combine the features back into the DataFrame\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def wavelet_transform_features(data, wavelet='haar', max_level=5):\n",
    "    \"\"\"\n",
    "    Perform wavelet transform and extract features from the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        data (str): Binary ciphertext string.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted wavelet features.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return {}\n",
    "\n",
    "    # Convert binary string to numeric values (1 for '1', -1 for '0')\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in data])\n",
    "\n",
    "    # Perform wavelet transform\n",
    "    coeffs = pywt.wavedec(numeric_data, wavelet, level=max_level)\n",
    "\n",
    "    # Extract features from each level\n",
    "    features = {}\n",
    "    for level, coeff in enumerate(coeffs):\n",
    "        features[f'wavelet_mean_level_{level}'] = np.mean(coeff)\n",
    "        features[f'wavelet_std_level_{level}'] = np.std(coeff)\n",
    "        features[f'wavelet_energy_level_{level}'] = np.sum(np.square(coeff))\n",
    "        features[f'wavelet_entropy_level_{level}'] = -np.sum(coeff * np.log2(np.abs(coeff + 1e-10)))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 {'wavelet_mean_level_0': -0.12626906806902638, 'wavelet_std_level_0': 1.1922902844731145, 'wavelet_energy_level_0': 20.125000000000007, 'wavelet_entropy_level_0': 1.0218964082766577, 'wavelet_mean_level_1': 0.47982245866230017, 'wavelet_std_level_1': 0.922023617387542, 'wavelet_energy_level_1': 15.125000000000004, 'wavelet_entropy_level_1': -5.57364988202677, 'wavelet_mean_level_2': -0.055555555555555566, 'wavelet_std_level_2': 0.9060836905722274, 'wavelet_energy_level_2': 22.25000000000001, 'wavelet_entropy_level_2': -0.7950452354883194, 'wavelet_mean_level_3': 0.0933914616661478, 'wavelet_std_level_3': 1.0003559352310616, 'wavelet_energy_level_3': 53.500000000000014, 'wavelet_entropy_level_3': -9.906406914228153, 'wavelet_mean_level_4': 0.0471698113207547, 'wavelet_std_level_4': 0.9941534321400364, 'wavelet_energy_level_4': 105.00000000000003, 'wavelet_entropy_level_4': 5.99999999047821, 'wavelet_mean_level_5': 0.06032190550406568, 'wavelet_std_level_5': 1.0052757219763302, 'wavelet_energy_level_5': 214.00000000000006, 'wavelet_entropy_level_5': -6.363961046115771}\n",
      "466 {'wavelet_mean_level_0': -0.23570226039551587, 'wavelet_std_level_0': 0.8628119403696525, 'wavelet_energy_level_0': 12.000000000000004, 'wavelet_entropy_level_0': 4.472975223305393, 'wavelet_mean_level_1': -0.18856180831641278, 'wavelet_std_level_1': 1.315716957066037, 'wavelet_energy_level_1': 26.500000000000014, 'wavelet_entropy_level_1': -2.2502004473840103, 'wavelet_mean_level_2': 0.1666666666666667, 'wavelet_std_level_2': 0.74535599249993, 'wavelet_energy_level_2': 17.500000000000007, 'wavelet_entropy_level_2': -2.1323312558420553, 'wavelet_mean_level_3': -0.14381832837692496, 'wavelet_std_level_3': 0.945817384969743, 'wavelet_energy_level_3': 54.00000000000002, 'wavelet_entropy_level_3': 4.776426752872732, 'wavelet_mean_level_4': -0.1538461538461539, 'wavelet_std_level_4': 0.9924103836083913, 'wavelet_energy_level_4': 118.00000000000004, 'wavelet_entropy_level_4': 3.9999999899011405, 'wavelet_mean_level_5': 0.09711337767368895, 'wavelet_std_level_5': 1.0354294066977672, 'wavelet_energy_level_5': 252.00000000000006, 'wavelet_entropy_level_5': -11.313708517162727}\n",
      "471 {'wavelet_mean_level_0': -7.401486830834377e-18, 'wavelet_std_level_0': 0.670820393249937, 'wavelet_energy_level_0': 6.750000000000002, 'wavelet_entropy_level_0': -1.7312340450104102e-09, 'wavelet_mean_level_1': 2.960594732333751e-17, 'wavelet_std_level_1': 1.0567244989431572, 'wavelet_energy_level_1': 16.750000000000004, 'wavelet_entropy_level_1': 2.294641051202193, 'wavelet_mean_level_2': -0.2666666666666668, 'wavelet_std_level_2': 0.8439325934114779, 'wavelet_energy_level_2': 23.500000000000007, 'wavelet_entropy_level_2': 3.887218751946207, 'wavelet_mean_level_3': 0.011984860698077075, 'wavelet_std_level_3': 0.8284304684644027, 'wavelet_energy_level_3': 40.500000000000014, 'wavelet_entropy_level_3': -4.95665944563378, 'wavelet_mean_level_4': -0.0593220338983051, 'wavelet_std_level_4': 1.144569883549304, 'wavelet_energy_level_4': 155.00000000000006, 'wavelet_entropy_level_4': 3.9999999880256323, 'wavelet_mean_level_5': 0.06591673383942392, 'wavelet_std_level_5': 0.985003175317919, 'wavelet_energy_level_5': 230.00000000000006, 'wavelet_entropy_level_5': -7.778174609643024}\n",
      "476 {'wavelet_mean_level_0': -0.518544972870135, 'wavelet_std_level_0': 0.693621734889494, 'wavelet_energy_level_0': 11.250000000000004, 'wavelet_entropy_level_0': 0.30911278129285363, 'wavelet_mean_level_1': -0.23570226039551578, 'wavelet_std_level_1': 1.400396769173334, 'wavelet_energy_level_1': 30.25000000000001, 'wavelet_entropy_level_1': 4.334962289059338, 'wavelet_mean_level_2': 0.03333333333333333, 'wavelet_std_level_2': 0.9213516640723504, 'wavelet_energy_level_2': 25.50000000000001, 'wavelet_entropy_level_2': -1.6724889870028625, 'wavelet_mean_level_3': 0.11785113019775793, 'wavelet_std_level_3': 1.1983785341498367, 'wavelet_energy_level_3': 87.00000000000003, 'wavelet_entropy_level_3': 0.5199621211964143, 'wavelet_mean_level_4': 0.07563025210084033, 'wavelet_std_level_4': 0.918160966696735, 'wavelet_energy_level_4': 101.00000000000003, 'wavelet_entropy_level_4': -2.000000010675951, 'wavelet_mean_level_5': -0.017826221374450786, 'wavelet_std_level_5': 0.9656372688930387, 'wavelet_energy_level_5': 222.00000000000006, 'wavelet_entropy_level_5': 2.1213203275457264}\n",
      "482 {'wavelet_mean_level_0': -0.5745242597140701, 'wavelet_std_level_0': 1.6243989272958788, 'wavelet_energy_level_0': 47.500000000000014, 'wavelet_entropy_level_0': 12.943462515829577, 'wavelet_mean_level_1': -0.044194173824159244, 'wavelet_std_level_1': 0.8279775812182358, 'wavelet_energy_level_1': 11.000000000000004, 'wavelet_entropy_level_1': 2.207980696659571, 'wavelet_mean_level_2': -0.35483870967741943, 'wavelet_std_level_2': 0.637453962753855, 'wavelet_energy_level_2': 16.500000000000007, 'wavelet_entropy_level_2': 0.37744374790780855, 'wavelet_mean_level_3': -0.046367657782724434, 'wavelet_std_level_3': 0.948414141909556, 'wavelet_energy_level_3': 55.00000000000002, 'wavelet_entropy_level_3': 2.828427119119681, 'wavelet_mean_level_4': -0.06611570247933886, 'wavelet_std_level_4': 0.951167505214814, 'wavelet_energy_level_4': 110.00000000000004, 'wavelet_entropy_level_4': -4.000000010675944, 'wavelet_mean_level_5': -0.08215348495113417, 'wavelet_std_level_5': 1.0591890814696756, 'wavelet_energy_level_5': 272.00000000000006, 'wavelet_entropy_level_5': 9.89949491699101}\n",
      "513 {'wavelet_mean_level_0': 0.1663780661615407, 'wavelet_std_level_0': 1.7452357293113883, 'wavelet_energy_level_0': 52.25000000000002, 'wavelet_entropy_level_0': -9.582500046604286, 'wavelet_mean_level_1': -0.12478354962115545, 'wavelet_std_level_1': 1.111048633383771, 'wavelet_energy_level_1': 21.250000000000007, 'wavelet_entropy_level_1': -0.2599810660804492, 'wavelet_mean_level_2': 0.0, 'wavelet_std_level_2': 0.896119580764924, 'wavelet_energy_level_2': 26.50000000000001, 'wavelet_entropy_level_2': -2.3774437542556637, 'wavelet_mean_level_3': -0.06527139518645055, 'wavelet_std_level_3': 1.0504436932376349, 'wavelet_energy_level_3': 72.00000000000003, 'wavelet_entropy_level_3': 9.379532801421108, 'wavelet_mean_level_4': -0.20155038759689928, 'wavelet_std_level_4': 0.959488275768422, 'wavelet_energy_level_4': 124.00000000000006, 'wavelet_entropy_level_4': 7.999999988169909, 'wavelet_mean_level_5': 0.11005553014576616, 'wavelet_std_level_5': 0.9761497451884195, 'wavelet_energy_level_5': 248.00000000000006, 'wavelet_entropy_level_5': -14.14213564162038}\n",
      "518 {'wavelet_mean_level_0': 0.020797258270192586, 'wavelet_std_level_0': 1.2454067858670665, 'wavelet_energy_level_0': 26.375000000000007, 'wavelet_entropy_level_0': -7.68805825475059, 'wavelet_mean_level_1': 0.020797258270192638, 'wavelet_std_level_1': 1.0107553104376228, 'wavelet_energy_level_1': 17.375000000000007, 'wavelet_entropy_level_1': -3.3656691847901192, 'wavelet_mean_level_2': 0.22727272727272732, 'wavelet_std_level_2': 0.9218797421318946, 'wavelet_energy_level_2': 29.75000000000001, 'wavelet_entropy_level_2': 5.122556244590179, 'wavelet_mean_level_3': -0.0652713951864506, 'wavelet_std_level_3': 1.0649888184539558, 'wavelet_energy_level_3': 74.00000000000003, 'wavelet_entropy_level_3': 4.603106042921867, 'wavelet_mean_level_4': -0.015384615384615379, 'wavelet_std_level_4': 1.0450220121687201, 'wavelet_energy_level_4': 142.00000000000006, 'wavelet_entropy_level_4': 3.999999987304282, 'wavelet_mean_level_5': -0.09828511244291782, 'wavelet_std_level_5': 0.9656215504829132, 'wavelet_energy_level_5': 244.00000000000006, 'wavelet_entropy_level_5': 12.727922043756976}\n",
      "525 {'wavelet_mean_level_0': -0.14558080789134809, 'wavelet_std_level_0': 1.12497597052384, 'wavelet_energy_level_0': 21.875000000000004, 'wavelet_entropy_level_0': 3.754530346327307, 'wavelet_mean_level_1': 0.2287698409721184, 'wavelet_std_level_1': 1.0285710852968069, 'wavelet_energy_level_1': 18.875000000000007, 'wavelet_entropy_level_1': -9.446692375419765, 'wavelet_mean_level_2': 0.09090909090909091, 'wavelet_std_level_2': 0.9083330174134464, 'wavelet_energy_level_2': 27.50000000000001, 'wavelet_entropy_level_2': -0.377443754977012, 'wavelet_mean_level_3': 0.10713739108887087, 'wavelet_std_level_3': 0.9474241333856658, 'wavelet_energy_level_3': 60.000000000000014, 'wavelet_entropy_level_3': -7.431533181085125, 'wavelet_mean_level_4': -0.045454545454545456, 'wavelet_std_level_4': 1.0214638661020108, 'wavelet_energy_level_4': 138.00000000000006, 'wavelet_entropy_level_4': 9.999999987448554, 'wavelet_mean_level_5': -0.0537723787974561, 'wavelet_std_level_5': 1.00045530267338, 'wavelet_energy_level_5': 264.00000000000006, 'wavelet_entropy_level_5': 7.071067792821896}\n",
      "527 {'wavelet_mean_level_0': -0.020797258270192572, 'wavelet_std_level_0': 0.8950072973354672, 'wavelet_energy_level_0': 13.625000000000005, 'wavelet_entropy_level_0': -1.712958390225475, 'wavelet_mean_level_1': -0.1039862913509629, 'wavelet_std_level_1': 0.8203720988812562, 'wavelet_energy_level_1': 11.625000000000004, 'wavelet_entropy_level_1': -0.9352360577735812, 'wavelet_mean_level_2': 0.5151515151515152, 'wavelet_std_level_2': 0.9413469434550952, 'wavelet_energy_level_2': 38.000000000000014, 'wavelet_entropy_level_2': -6.754887506491561, 'wavelet_mean_level_3': -0.0642824346533225, 'wavelet_std_level_3': 1.035193385002347, 'wavelet_energy_level_3': 71.00000000000003, 'wavelet_entropy_level_3': 0.707106773973073, 'wavelet_mean_level_4': -0.045454545454545456, 'wavelet_std_level_4': 1.0791666001893256, 'wavelet_energy_level_4': 154.00000000000006, 'wavelet_entropy_level_4': -1.3561333611633586e-08, 'wavelet_mean_level_5': 0.08570991287109668, 'wavelet_std_level_5': 0.9496024009692364, 'wavelet_energy_level_5': 240.00000000000006, 'wavelet_entropy_level_5': -11.313708516297108}\n",
      "529 {'wavelet_mean_level_0': 0.31195887405288875, 'wavelet_std_level_0': 1.3110522992954514, 'wavelet_energy_level_0': 30.875000000000007, 'wavelet_entropy_level_0': -5.263286377006988, 'wavelet_mean_level_1': -0.1039862913509629, 'wavelet_std_level_1': 1.343638091823579, 'wavelet_energy_level_1': 30.875000000000014, 'wavelet_entropy_level_1': 7.983569310869072, 'wavelet_mean_level_2': 0.16176470588235298, 'wavelet_std_level_2': 0.9680224553430788, 'wavelet_energy_level_2': 32.750000000000014, 'wavelet_entropy_level_2': -3.500000004328089, 'wavelet_mean_level_3': -0.03166149766506931, 'wavelet_std_level_3': 1.010636239834639, 'wavelet_energy_level_3': 68.50000000000003, 'wavelet_entropy_level_3': -1.060660179281837, 'wavelet_mean_level_4': 0.03759398496240603, 'wavelet_std_level_4': 0.9917404389807266, 'wavelet_energy_level_4': 131.00000000000006, 'wavelet_entropy_level_4': 3.999999988025627, 'wavelet_mean_level_5': -0.09072313418997215, 'wavelet_std_level_5': 0.9670395748560877, 'wavelet_energy_level_5': 250.00000000000006, 'wavelet_entropy_level_5': 12.020815262137619}\n",
      "536 {'wavelet_mean_level_0': -0.3535533905932739, 'wavelet_std_level_0': 1.0641207361838556, 'wavelet_energy_level_0': 21.375000000000007, 'wavelet_entropy_level_0': 4.686310416302536, 'wavelet_mean_level_1': -0.27036435751250343, 'wavelet_std_level_1': 0.8789148704402318, 'wavelet_energy_level_1': 14.375000000000007, 'wavelet_entropy_level_1': 1.1507765100812533, 'wavelet_mean_level_2': -0.2205882352941177, 'wavelet_std_level_2': 0.9715903917152314, 'wavelet_energy_level_2': 33.750000000000014, 'wavelet_entropy_level_2': 6.304820233034595, 'wavelet_mean_level_3': 0.03166149766506929, 'wavelet_std_level_3': 0.9882355437162299, 'wavelet_energy_level_3': 65.50000000000003, 'wavelet_entropy_level_3': -3.7157665946542435, 'wavelet_mean_level_4': -0.022388059701492536, 'wavelet_std_level_4': 1.0752744731896366, 'wavelet_energy_level_4': 155.00000000000006, 'wavelet_entropy_level_4': -2.0000000141384167, 'wavelet_mean_level_5': 0.05804607905262705, 'wavelet_std_level_5': 0.9563164227742877, 'wavelet_energy_level_5': 246.00000000000006, 'wavelet_entropy_level_5': -7.778174610797178}\n",
      "536 {'wavelet_mean_level_0': 0.12478354962115545, 'wavelet_std_level_0': 0.7168269180561143, 'wavelet_energy_level_0': 9.000000000000004, 'wavelet_entropy_level_0': 0.6204464243356203, 'wavelet_mean_level_1': 0.08318903308077032, 'wavelet_std_level_1': 1.2575034309665856, 'wavelet_energy_level_1': 27.00000000000001, 'wavelet_entropy_level_1': -2.3882133816300675, 'wavelet_mean_level_2': 0.02941176470588239, 'wavelet_std_level_2': 0.984745237810777, 'wavelet_energy_level_2': 33.000000000000014, 'wavelet_entropy_level_2': -4.182263991618341, 'wavelet_mean_level_3': 0.06332299533013858, 'wavelet_std_level_3': 0.903817304907623, 'wavelet_energy_level_3': 55.000000000000014, 'wavelet_entropy_level_3': -3.7226785652015217, 'wavelet_mean_level_4': 0.044776119402985086, 'wavelet_std_level_4': 1.0211617004834226, 'wavelet_energy_level_4': 140.00000000000006, 'wavelet_entropy_level_4': -12.000000011541568, 'wavelet_mean_level_5': 0.031661497665069295, 'wavelet_std_level_5': 1.0069373976071865, 'wavelet_energy_level_5': 272.00000000000006, 'wavelet_entropy_level_5': -4.2426407067399445}\n",
      "537 {'wavelet_mean_level_0': -0.2495670992423109, 'wavelet_std_level_0': 0.8381062852220594, 'wavelet_energy_level_0': 13.000000000000007, 'wavelet_entropy_level_0': 3.1888924851653986, 'wavelet_mean_level_1': 0.2911616157826961, 'wavelet_std_level_1': 0.8595081880119095, 'wavelet_energy_level_1': 14.000000000000007, 'wavelet_entropy_level_1': -6.708381591571866, 'wavelet_mean_level_2': -0.029411764705882366, 'wavelet_std_level_2': 0.984745237810777, 'wavelet_energy_level_2': 33.00000000000001, 'wavelet_entropy_level_2': 0.3774437463208411, 'wavelet_mean_level_3': -0.06239177481057772, 'wavelet_std_level_3': 0.9528231589955922, 'wavelet_energy_level_3': 62.00000000000002, 'wavelet_entropy_level_3': 7.431533167956597, 'wavelet_mean_level_4': -0.02962962962962964, 'wavelet_std_level_4': 1.0395208350627192, 'wavelet_energy_level_4': 146.00000000000006, 'wavelet_entropy_level_4': 9.999999988025632, 'wavelet_mean_level_5': -0.04205839590700654, 'wavelet_std_level_5': 1.0120540480460731, 'wavelet_energy_level_5': 276.00000000000006, 'wavelet_entropy_level_5': 5.6568542295831845}\n",
      "539 {'wavelet_mean_level_0': 0.540728715025007, 'wavelet_std_level_0': 1.144794144472269, 'wavelet_energy_level_0': 27.25000000000001, 'wavelet_entropy_level_0': -12.632128877114901, 'wavelet_mean_level_1': 0.20797258270192578, 'wavelet_std_level_1': 1.2488749262095382, 'wavelet_energy_level_1': 27.250000000000014, 'wavelet_entropy_level_1': -3.094069440099573, 'wavelet_mean_level_2': -0.14705882352941177, 'wavelet_std_level_2': 0.9512330369880138, 'wavelet_energy_level_2': 31.50000000000001, 'wavelet_entropy_level_2': 1.2646625023065907, 'wavelet_mean_level_3': -0.04159451654038516, 'wavelet_std_level_3': 1.0563697056510575, 'wavelet_energy_level_3': 76.00000000000003, 'wavelet_entropy_level_3': -3.362213202762541, 'wavelet_mean_level_4': 0.0740740740740741, 'wavelet_std_level_4': 0.9785070232313265, 'wavelet_energy_level_4': 130.00000000000006, 'wavelet_entropy_level_4': -6.000000012262915, 'wavelet_mean_level_5': 0.031426968052735454, 'wavelet_std_level_5': 0.9655804881066858, 'wavelet_energy_level_5': 252.00000000000006, 'wavelet_entropy_level_5': -4.242640705297249}\n",
      "540 {'wavelet_mean_level_0': -0.2911616157826961, 'wavelet_std_level_0': 0.7973789242191742, 'wavelet_energy_level_0': 12.250000000000007, 'wavelet_entropy_level_0': 2.5684460589542857, 'wavelet_mean_level_1': 0.45753968194423666, 'wavelet_std_level_1': 0.9908753251719423, 'wavelet_energy_level_1': 20.250000000000007, 'wavelet_entropy_level_1': -4.863087115638715, 'wavelet_mean_level_2': -0.647058823529412, 'wavelet_std_level_2': 1.046910101797511, 'wavelet_energy_level_2': 51.50000000000002, 'wavelet_entropy_level_2': 20.190270200195172, 'wavelet_mean_level_3': -0.1663780661615406, 'wavelet_std_level_3': 0.8754632980080718, 'wavelet_energy_level_3': 54.000000000000014, 'wavelet_entropy_level_3': 6.19064031466875, 'wavelet_mean_level_4': -0.02962962962962963, 'wavelet_std_level_4': 1.0106158059669528, 'wavelet_energy_level_4': 138.00000000000006, 'wavelet_entropy_level_4': -1.2984256336778799e-08, 'wavelet_mean_level_5': 3.2895497025930565e-18, 'wavelet_std_level_5': 0.9888264649460885, 'wavelet_energy_level_5': 264.00000000000006, 'wavelet_entropy_level_5': -1.904357982418503e-08}\n",
      "546 {'wavelet_mean_level_0': -0.4517626657580721, 'wavelet_std_level_0': 1.526640991212205, 'wavelet_energy_level_0': 45.625000000000014, 'wavelet_entropy_level_0': 14.628025099887635, 'wavelet_mean_level_1': 0.09820927516479833, 'wavelet_std_level_1': 0.911390539806829, 'wavelet_energy_level_1': 15.125000000000007, 'wavelet_entropy_level_1': -1.314964392207282, 'wavelet_mean_level_2': -0.10000000000000005, 'wavelet_std_level_2': 0.6845227743263396, 'wavelet_energy_level_2': 16.750000000000007, 'wavelet_entropy_level_2': 0.8774437468979204, 'wavelet_mean_level_3': 0.010247924365022427, 'wavelet_std_level_3': 0.9742537361309167, 'wavelet_energy_level_3': 65.50000000000001, 'wavelet_entropy_level_3': 6.544193704829211, 'wavelet_mean_level_4': 0.021897810218978103, 'wavelet_std_level_4': 0.9159347926035862, 'wavelet_energy_level_4': 115.00000000000006, 'wavelet_entropy_level_4': 5.9999999899011325, 'wavelet_mean_level_5': -0.08806458080711582, 'wavelet_std_level_5': 1.07567643575679, 'wavelet_energy_level_5': 318.0000000000001, 'wavelet_entropy_level_5': 12.020815257232453}\n",
      "560 {'wavelet_mean_level_0': -0.2160604053625563, 'wavelet_std_level_0': 1.2198525006970273, 'wavelet_energy_level_0': 27.62500000000001, 'wavelet_entropy_level_0': 0.5804319404237218, 'wavelet_mean_level_1': 0.01964185503295969, 'wavelet_std_level_1': 1.120963265221171, 'wavelet_energy_level_1': 22.625000000000007, 'wavelet_entropy_level_1': -6.411857625902204, 'wavelet_mean_level_2': -0.08571428571428567, 'wavelet_std_level_2': 0.7881779556647842, 'wavelet_energy_level_2': 22.000000000000007, 'wavelet_entropy_level_2': 0.7548874985567311, 'wavelet_mean_level_3': 0.060609152673132625, 'wavelet_std_level_3': 0.9981615754036242, 'wavelet_energy_level_3': 70.00000000000003, 'wavelet_entropy_level_3': -1.5875342796817042, 'wavelet_mean_level_4': 0.1571428571428572, 'wavelet_std_level_4': 0.9355779617161681, 'wavelet_energy_level_4': 126.00000000000006, 'wavelet_entropy_level_4': -12.000000012118647, 'wavelet_mean_level_5': 0.010101525445522107, 'wavelet_std_level_5': 1.0211538091986028, 'wavelet_energy_level_5': 292.0000000000001, 'wavelet_entropy_level_5': -1.4142135834364495}\n",
      "563 {'wavelet_mean_level_0': -0.05892556509887898, 'wavelet_std_level_0': 0.7383352144445563, 'wavelet_energy_level_0': 9.875000000000004, 'wavelet_entropy_level_0': -0.5234181156311741, 'wavelet_mean_level_1': 0.05892556509887893, 'wavelet_std_level_1': 1.2204848763230678, 'wavelet_energy_level_1': 26.875000000000014, 'wavelet_entropy_level_1': -0.4367577616656253, 'wavelet_mean_level_2': 0.013888888888888902, 'wavelet_std_level_2': 1.1394815531093707, 'wavelet_energy_level_2': 46.75000000000002, 'wavelet_entropy_level_2': 1.6951797588863164, 'wavelet_mean_level_3': 0.10955175483171864, 'wavelet_std_level_3': 1.0253675900395098, 'wavelet_energy_level_3': 75.50000000000003, 'wavelet_entropy_level_3': -6.5441937192561666, 'wavelet_mean_level_4': 0.1418439716312057, 'wavelet_std_level_4': 0.9115613624430997, 'wavelet_energy_level_4': 120.00000000000003, 'wavelet_entropy_level_4': -4.000000012118649, 'wavelet_mean_level_5': 0.07020918394760048, 'wavelet_std_level_5': 1.0081404992530025, 'wavelet_energy_level_5': 288.0000000000001, 'wavelet_entropy_level_5': -9.899494957386484}\n",
      "584 {'wavelet_mean_level_0': 0.11164843913471804, 'wavelet_std_level_0': 0.9531984355934127, 'wavelet_energy_level_0': 17.500000000000007, 'wavelet_entropy_level_0': -3.3108606194523733, 'wavelet_mean_level_1': 0.14886458551295742, 'wavelet_std_level_1': 1.0151211327361007, 'wavelet_energy_level_1': 20.000000000000007, 'wavelet_entropy_level_1': 1.8897350811940241, 'wavelet_mean_level_2': 0.16216216216216225, 'wavelet_std_level_2': 1.0465777824817968, 'wavelet_energy_level_2': 41.500000000000014, 'wavelet_entropy_level_2': -9.427376490320487, 'wavelet_mean_level_3': -0.05811836557697651, 'wavelet_std_level_3': 0.9271655484393271, 'wavelet_energy_level_3': 63.000000000000014, 'wavelet_entropy_level_3': -4.0693199845261665, 'wavelet_mean_level_4': -0.10958904109589043, 'wavelet_std_level_4': 0.9939769826669447, 'wavelet_energy_level_4': 146.00000000000006, 'wavelet_entropy_level_4': 11.999999985861594, 'wavelet_mean_level_5': 0.05811836557697651, 'wavelet_std_level_5': 1.0051471960462544, 'wavelet_energy_level_5': 296.0000000000001, 'wavelet_entropy_level_5': -8.485281395590466}\n",
      "599 {'wavelet_mean_level_0': 0.18608073189119678, 'wavelet_std_level_0': 0.7197260174303867, 'wavelet_energy_level_0': 10.500000000000004, 'wavelet_entropy_level_0': 0.0513525769623111, 'wavelet_mean_level_1': 0.29772917102591484, 'wavelet_std_level_1': 0.8680222369721393, 'wavelet_energy_level_1': 16.000000000000007, 'wavelet_entropy_level_1': -1.9981014930477536, 'wavelet_mean_level_2': 0.026315789473684226, 'wavelet_std_level_2': 1.044707012698906, 'wavelet_energy_level_2': 41.500000000000014, 'wavelet_entropy_level_2': 5.314595237072989, 'wavelet_mean_level_3': -0.028284271247461915, 'wavelet_std_level_3': 1.1041738993473813, 'wavelet_energy_level_3': 91.50000000000003, 'wavelet_entropy_level_3': 7.958407265903883, 'wavelet_mean_level_4': 0.04666666666666668, 'wavelet_std_level_4': 1.002241931316431, 'wavelet_energy_level_4': 151.00000000000006, 'wavelet_entropy_level_4': 3.9999999868714715, 'wavelet_mean_level_5': -0.0801387685344754, 'wavelet_std_level_5': 0.9799206317066932, 'wavelet_energy_level_5': 290.0000000000001, 'wavelet_entropy_level_5': 12.020815259252231}\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),wavelet_transform_features(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def wavelet_transform_features(data, wavelet='haar', max_level=5):\n",
    "    \"\"\"\n",
    "    Perform wavelet transform and extract features from the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        data (str): Binary ciphertext string.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted wavelet features.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return {}\n",
    "\n",
    "    # Convert binary string to numeric values (1 for '1', -1 for '0')\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in data])\n",
    "\n",
    "    # Perform wavelet transform\n",
    "    coeffs = pywt.wavedec(numeric_data, wavelet, level=max_level)\n",
    "\n",
    "    # Extract features from each level\n",
    "    features = {}\n",
    "    for level, coeff in enumerate(coeffs):\n",
    "        features[f'wavelet_mean_level_{level}'] = np.mean(coeff)\n",
    "        features[f'wavelet_std_level_{level}'] = np.std(coeff)\n",
    "        features[f'wavelet_energy_level_{level}'] = np.sum(np.square(coeff))\n",
    "        features[f'wavelet_entropy_level_{level}'] = -np.sum(coeff * np.log2(np.abs(coeff + 1e-10)))\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_features_for_row(row, segment_sizes, wavelet='haar', max_level=4):\n",
    "    \"\"\"\n",
    "    Extract wavelet features for a single row of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataset containing binary ciphertext in column 4.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of features extracted for the row.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    binary_string = row[4]  # Column containing binary ciphertext\n",
    "\n",
    "    # Extract features for each segment size\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]  # Binary segment (first `size` bits)\n",
    "\n",
    "        # Pad with zeros if the segment is smaller than the required size\n",
    "        segment = segment.ljust(size, '0')\n",
    "\n",
    "        # Extract wavelet features for the segment\n",
    "        wavelet_features = wavelet_transform_features(segment, wavelet=wavelet, max_level=max_level)\n",
    "        features.update({f\"{key}_{size}\": value for key, value in wavelet_features.items()})\n",
    "\n",
    "    return features\n",
    "\n",
    "def append_features_to_dataset(dataset, segment_sizes=[128, 256, 512, 1024, 2048, 4096], wavelet='haar', max_level=4, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Append wavelet transform features to the dataset for each ciphertext in column 4 using parallelization.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "        n_jobs (int): Number of parallel jobs (-1 means using all available processors).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with appended feature columns.\n",
    "    \"\"\"\n",
    "    # Extract features in parallel for each row\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes, wavelet=wavelet, max_level=max_level)\n",
    "        for _, row in dataset.iterrows()\n",
    "    )\n",
    "\n",
    "    # Combine the features back into the DataFrame\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "    return updated_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "def dominant_frequency_magnitude(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    fft_values = np.abs(fft(numeric_data))\n",
    "    return np.max(fft_values)\n",
    "\n",
    "def power_spectral_density_peaks(binary_string, num_peaks=2):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    fft_values = np.abs(fft(numeric_data)) ** 2\n",
    "    return sorted(fft_values, reverse=True)[:num_peaks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 48.58690447561001\n",
      "466 52.93317674818621\n",
      "471 50.159791805455164\n",
      "476 46.587455112944724\n",
      "482 56.291034996799645\n",
      "513 56.24880940007889\n",
      "518 68.57538128365016\n",
      "525 53.93730411429009\n",
      "527 48.74017019220805\n",
      "529 56.011278394961174\n",
      "536 65.41078542520933\n",
      "536 52.6147584414381\n",
      "537 51.86528225249833\n",
      "539 52.137809043133856\n",
      "540 53.89408602244138\n",
      "546 55.51109945646693\n",
      "560 50.871442074592856\n",
      "563 66.2080741099716\n",
      "584 57.28672990552335\n",
      "599 60.64463614682276\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),dominant_frequency_magnitude(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 [2360.687286522052, 2360.687286522052]\n",
      "466 [2801.9212006547214, 2801.9212006547214]\n",
      "471 [2516.004713966607, 2516.004713966607]\n",
      "476 [2170.3909739006394, 2170.3909739006394]\n",
      "482 [3168.6806210109226, 3168.6806210109226]\n",
      "513 [3163.928558926403, 3163.928558926403]\n",
      "518 [4702.582918197996, 4702.582918197996]\n",
      "525 [2909.232775117415, 2909.232775117415]\n",
      "527 [2375.604190365406, 2375.604190365406]\n",
      "529 [3137.2633074378446, 3137.2633074378446]\n",
      "536 [4278.5708499427765, 4278.5708499427765]\n",
      "536 [2768.312805850882, 2768.312805850882]\n",
      "537 [2690.0075031313186, 2690.0075031313186]\n",
      "539 [2718.3511318182905, 2718.3511318182905]\n",
      "540 [2904.572508194311, 2904.572508194311]\n",
      "546 [3081.4821628657633, 3081.4821628657633]\n",
      "560 [2587.9036187486563, 2587.9036187486563]\n",
      "563 [4383.509077351492, 4383.509077351492]\n",
      "584 [3281.7694232683834, 3281.7694232683834]\n",
      "599 [3677.7718933805218, 3677.7718933805218]\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),power_spectral_density_peaks(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy_gradient(binary_string, block_size=8):\n",
    "    \"\"\"\n",
    "    Compute the gradient of block-wise entropies and return the mean.\n",
    "\n",
    "    Parameters:\n",
    "        binary_string (str): The binary string to analyze.\n",
    "        block_size (int): The size of each block for entropy calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean of the gradients of block-wise entropies.\n",
    "    \"\"\"\n",
    "    entropies = []\n",
    "    # Iterate through the binary string in blocks\n",
    "    for i in range(0, len(binary_string), block_size):\n",
    "        block = binary_string[i:i + block_size]\n",
    "        counts = np.bincount([int(bit) for bit in block], minlength=2)\n",
    "        probabilities = counts / counts.sum()\n",
    "        # Compute entropy for the block\n",
    "        entropies.append(-np.sum(probabilities * np.log2(probabilities + 1e-10)))\n",
    "    \n",
    "    # Compute the gradient of the entropies\n",
    "    gradient = np.gradient(entropies)\n",
    "    \n",
    "    # Return the mean of the gradients\n",
    "    return np.mean(gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.0009582420651403136\n",
      "466 -0.02542372880989145\n",
      "471 -0.003093833027331119\n",
      "476 0.006692842031488411\n",
      "482 -0.025763572770762487\n",
      "513 -0.013644993904860098\n",
      "518 -0.006975206589146962\n",
      "525 -0.00774952720362712\n",
      "527 -0.00033572418103973813\n",
      "529 -0.011829308448439895\n",
      "536 -1.6570492904853082e-18\n",
      "536 0.004565161370877885\n",
      "537 -0.022106307991234697\n",
      "539 -0.021053691237809456\n",
      "540 0.00449802664483556\n",
      "546 -0.02173913043164631\n",
      "560 0.00032547140767882107\n",
      "563 0.00024465754827046057\n",
      "584 1.5208534583906253e-18\n",
      "599 0.006391122507340691\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),entropy_gradient(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacent_bit_probability(binary_string):\n",
    "    transitions = [1 if binary_string[i] != binary_string[i + 1] else 0 for i in range(len(binary_string) - 1)]\n",
    "    return sum(transitions) / len(transitions)\n",
    "\n",
    "def bit_pair_frequency(binary_string):\n",
    "    pairs = [binary_string[i:i + 2] for i in range(len(binary_string) - 1)]\n",
    "    return {pair: pairs.count(pair) / len(pairs) for pair in ['00', '01', '10', '11']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.530952380952381\n",
      "466 0.546236559139785\n",
      "471 0.49148936170212765\n",
      "476 0.4357894736842105\n",
      "482 0.5446985446985447\n",
      "513 0.4765625\n",
      "518 0.4816247582205029\n",
      "525 0.4866412213740458\n",
      "527 0.4828897338403042\n",
      "529 0.48484848484848486\n",
      "536 0.4822429906542056\n",
      "536 0.49906542056074765\n",
      "537 0.503731343283582\n",
      "539 0.5\n",
      "540 0.47309833024118736\n",
      "546 0.5431192660550459\n",
      "560 0.5098389982110912\n",
      "563 0.5302491103202847\n",
      "584 0.5214408233276158\n",
      "599 0.4866220735785953\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),adjacent_bit_probability(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 {'00': 0.22857142857142856, '01': 0.2642857142857143, '10': 0.26666666666666666, '11': 0.24047619047619048}\n",
      "466 {'00': 0.23225806451612904, '01': 0.2731182795698925, '10': 0.2731182795698925, '11': 0.221505376344086}\n",
      "471 {'00': 0.2553191489361702, '01': 0.24468085106382978, '10': 0.24680851063829787, '11': 0.2531914893617021}\n",
      "476 {'00': 0.3263157894736842, '01': 0.2168421052631579, '10': 0.21894736842105264, '11': 0.23789473684210527}\n",
      "482 {'00': 0.2494802494802495, '01': 0.27234927234927236, '10': 0.27234927234927236, '11': 0.20582120582120583}\n",
      "513 {'00': 0.27734375, '01': 0.23828125, '10': 0.23828125, '11': 0.24609375}\n",
      "518 {'00': 0.2765957446808511, '01': 0.24177949709864605, '10': 0.23984526112185686, '11': 0.24177949709864605}\n",
      "525 {'00': 0.26145038167938933, '01': 0.24236641221374045, '10': 0.24427480916030533, '11': 0.25190839694656486}\n",
      "527 {'00': 0.26425855513307983, '01': 0.2414448669201521, '10': 0.2414448669201521, '11': 0.25285171102661597}\n",
      "529 {'00': 0.24431818181818182, '01': 0.24242424242424243, '10': 0.24242424242424243, '11': 0.2708333333333333}\n",
      "536 {'00': 0.29158878504672897, '01': 0.2411214953271028, '10': 0.2411214953271028, '11': 0.2261682242990654}\n",
      "536 {'00': 0.23925233644859814, '01': 0.2485981308411215, '10': 0.2504672897196262, '11': 0.2616822429906542}\n",
      "537 {'00': 0.2630597014925373, '01': 0.251865671641791, '10': 0.251865671641791, '11': 0.2332089552238806}\n",
      "539 {'00': 0.20631970260223048, '01': 0.25092936802973975, '10': 0.24907063197026022, '11': 0.2936802973977695}\n",
      "540 {'00': 0.2894248608534323, '01': 0.23562152133580705, '10': 0.23747680890538034, '11': 0.23747680890538034}\n",
      "546 {'00': 0.24220183486238533, '01': 0.27155963302752295, '10': 0.27155963302752295, '11': 0.21467889908256882}\n",
      "560 {'00': 0.2629695885509839, '01': 0.25402504472271914, '10': 0.2558139534883721, '11': 0.22719141323792486}\n",
      "563 {'00': 0.23309608540925267, '01': 0.26512455516014233, '10': 0.26512455516014233, '11': 0.23665480427046262}\n",
      "584 {'00': 0.22984562607204118, '01': 0.2607204116638079, '10': 0.2607204116638079, '11': 0.24871355060034306}\n",
      "599 {'00': 0.2408026755852843, '01': 0.24247491638795987, '10': 0.24414715719063546, '11': 0.2725752508361204}\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),bit_pair_frequency(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nolds\n",
    "\n",
    "def hurst_exponent(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    return nolds.hurst_rs(numeric_data)\n",
    "\n",
    "def detrended_fluctuation_analysis(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    return nolds.dfa(numeric_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.451504018026283\n",
      "466 0.45218954342668216\n",
      "471 0.4112953088709478\n",
      "476 0.4765142712653743\n",
      "482 0.4246402186360491\n",
      "513 0.5276862898373157\n",
      "518 0.45460150635658136\n",
      "525 0.5523932616646413\n",
      "527 0.4365876447160413\n",
      "529 0.5363352983721582\n",
      "536 0.5061970306802395\n",
      "536 0.561175566189562\n",
      "537 0.4422256961056633\n",
      "539 0.5868517665104894\n",
      "540 0.4994815168375852\n",
      "546 0.49435377720276946\n",
      "560 0.5412103847097579\n",
      "563 0.5700454233621264\n",
      "584 0.45114338016094857\n",
      "599 0.46255329191055283\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),hurst_exponent(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.5256458666007027\n",
      "466 0.4890585121277919\n",
      "471 0.5235621342813578\n",
      "476 0.5986779904199686\n",
      "482 0.5123345310727101\n",
      "513 0.5880119823076552\n",
      "518 0.538625621911896\n",
      "525 0.5505531805421912\n",
      "527 0.5463731502426427\n",
      "529 0.5984492247514286\n",
      "536 0.5407580591573772\n",
      "536 0.5857327445322504\n",
      "537 0.47529948065818806\n",
      "539 0.6461664686341632\n",
      "540 0.5773270975033333\n",
      "546 0.49125502934816595\n",
      "560 0.5671933714127421\n",
      "563 0.5456470318539103\n",
      "584 0.5273820207914597\n",
      "599 0.48510722709493753\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),detrended_fluctuation_analysis(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyunicorn.timeseries.recurrence_plot import RecurrencePlot\n",
    "\n",
    "def recurrence_quantification_analysis(binary_string, threshold=0.5):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    rp = RecurrencePlot(numeric_data, threshold=threshold)\n",
    "    return rp.recurrence_rate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "421 0.5000705254427588\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "466 0.5000828897198327\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "471 0.500002253866508\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "476 0.5038927335640139\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "482 0.5010416487319433\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "513 0.5004274819602613\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "518 0.5006037477079948\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "525 0.5000453514739229\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "527 0.5000450079033878\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "529 0.5004020140008076\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "536 0.5020118623301404\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "536 0.5002506126085988\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "537 0.5005010940843155\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "539 0.5038017905762406\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "540 0.5013443072702332\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "546 0.5004293630667257\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "560 0.5006377551020408\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "563 0.5000015774413271\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "584 0.5002111090260837\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "599 0.5005030643727303\n"
     ]
    }
   ],
   "source": [
    "for k in random_binary_strings:\n",
    "    print(len(k),recurrence_quantification_analysis(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Function: Wavelet Transform Features\n",
    "def wavelet_transform_features(data, wavelet='haar', max_level=5):\n",
    "    \"\"\"\n",
    "    Perform wavelet transform and extract features from the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        data (str): Binary ciphertext string.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted wavelet features.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return {}\n",
    "\n",
    "    # Convert binary string to numeric values (1 for '1', -1 for '0')\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in data])\n",
    "\n",
    "    # Perform wavelet transform\n",
    "    coeffs = pywt.wavedec(numeric_data, wavelet, level=max_level)\n",
    "\n",
    "    # Extract features from each level\n",
    "    features = {}\n",
    "    for level, coeff in enumerate(coeffs):\n",
    "        features[f'wavelet_mean_level_{level}'] = np.mean(coeff)\n",
    "        features[f'wavelet_std_level_{level}'] = np.std(coeff)\n",
    "        features[f'wavelet_energy_level_{level}'] = np.sum(np.square(coeff))\n",
    "        features[f'wavelet_entropy_level_{level}'] = -np.sum(coeff * np.log2(np.abs(coeff + 1e-10)))\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function: Extract Features for a Single Row\n",
    "def extract_features_for_row(row, segment_sizes):\n",
    "    \"\"\"\n",
    "    Extract features for a single row of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataset containing binary ciphertext in column 4.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of features extracted for the row.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    binary_string = row[4]  # Column containing binary ciphertext\n",
    "\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]\n",
    "        segment = segment.ljust(size, '0')  # Pad with zeros if smaller than size\n",
    "\n",
    "        # Features from first function set\n",
    "        features[f\"entropy_{size}\"] = entropy_analysis(segment)\n",
    "        features[f\"frequency_deviation_{size}\"] = frequency_test(segment)\n",
    "        features[f\"mean_run_0_{size}\"], features[f\"mean_run_1_{size}\"] = run_length(segment)\n",
    "        features[f\"hamming_weight_{size}\"] = hamming_weight(segment)\n",
    "        features[f\"bit_transitions_{size}\"] = bit_transition(segment)\n",
    "        features[f\"skewness_{size}\"], features[f\"kurtosis_{size}\"] = binary_skewness_kurtosis(segment)\n",
    "        features[f\"autocorrelation_{size}\"] = autocorrelation(segment, lag=4)\n",
    "        features[f\"fractal_dimension_{size}\"] = fractal_dimension(segment)\n",
    "        spectral_features = extract_features(segment)\n",
    "        features[f\"spectral_mean_{size}\"] = spectral_features[0]\n",
    "        features[f\"spectral_variance_{size}\"] = spectral_features[1]\n",
    "        convolution_result = compute_convolution([int(bit) for bit in segment])\n",
    "        features[f\"convolution_{size}\"] = sum(convolution_result)\n",
    "        features[f\"complexity_{size}\"] = compute_ciphertext_complexity(segment)\n",
    "\n",
    "        # Features from second function set\n",
    "        features[f\"chi_square_{size}\"] = chi_square_test(segment)\n",
    "\n",
    "        # Features from third function set\n",
    "        features[f\"recurrence_rate_{size}\"] = recurrence_quantification_analysis(segment)\n",
    "        features[f\"hurst_exponent_{size}\"] = hurst_exponent(segment)\n",
    "        features[f\"dfa_{size}\"] = detrended_fluctuation_analysis(segment)\n",
    "        bit_freq = bit_pair_frequency(segment)\n",
    "        for pair, freq in bit_freq.items():\n",
    "            features[f\"bit_pair_freq_{pair}_{size}\"] = freq\n",
    "        features[f\"adjacent_bit_probability_{size}\"] = adjacent_bit_probability(segment)\n",
    "        features[f\"spectral_entropy_{size}\"] = spectral_entropy(segment)\n",
    "        features[f\"dominant_frequency_magnitude_{size}\"] = dominant_frequency_magnitude(segment)\n",
    "        peaks = power_spectral_density_peaks(segment)\n",
    "        for i, peak in enumerate(peaks):\n",
    "            features[f\"power_spectral_peak_{i + 1}_{size}\"] = peak\n",
    "\n",
    "        # Wavelet transform features\n",
    "        wavelet_features = wavelet_transform_features(segment)\n",
    "        for key, value in wavelet_features.items():\n",
    "            features[f\"{key}_{size}\"] = value\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function: Append Features to Dataset\n",
    "def append_features_to_dataset(dataset, segment_sizes=[128, 256, 512, 1024, 2048, 4096], n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Append features to the dataset for each ciphertext in column 4 using parallelization.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        n_jobs (int): Number of parallel jobs (-1 means using all available processors).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with appended feature columns.\n",
    "    \"\"\"\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes) for _, row in dataset.iterrows()\n",
    "    )\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "    return updated_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
