{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_str(k):\n",
    "    binary_string_final = \"\"\n",
    "    for hex_string in k:\n",
    "        pairs = [hex_string[i:i+2] for i in range(0, len(hex_string), 2)]\n",
    "        binary_list = [bin(int(pair, 16))[2:].zfill(8) for pair in pairs]\n",
    "        binary_string = ''.join(binary_list)\n",
    "        binary_string_final += binary_string\n",
    "    return binary_string_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_analysis(k):\n",
    "  #this function will be used to measure the randomness of cipher text this feature\n",
    "  # might be a little less useful\n",
    "  prob_0 = k.count('0') / len(k)\n",
    "  prob_1 = k.count('1') / len(k)\n",
    "\n",
    "    # Avoid log(0) by ensuring probabilities > 0\n",
    "  entropy = 0\n",
    "  if prob_0 > 0:\n",
    "      entropy -= prob_0 * np.log2(prob_0)\n",
    "  if prob_1 > 0:\n",
    "      entropy -= prob_1 * np.log2(prob_1)\n",
    "\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_test(k):\n",
    "  #checks the deviation form stability\n",
    "    E_i = 0.5\n",
    "    prob_0 = k.count('0') / len(k)\n",
    "    prob_1 = k.count('1') / len(k)\n",
    "    chi_squared = (((prob_0 - E_i) ** 2) / E_i) + (((prob_1 - E_i) ** 2) / E_i)\n",
    "    deviation = chi_squared ** 0.5  # Taking the square root for stability test\n",
    "\n",
    "    return deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length(k):\n",
    "  #used to find the A run of length k\n",
    "  #consists of exactly k identical bits and\n",
    "  #is bounded before and after with a bit of opposite value.\n",
    "  runs = [(key, len(list(group))) for key, group in groupby(k)]\n",
    "\n",
    "    # Separate runs of 0s and 1s\n",
    "  run_lengths_0 = [length for bit, length in runs if bit == '0']\n",
    "  run_lengths_1 = [length for bit, length in runs if bit == '1']\n",
    "\n",
    "    # Calculate mean run lengths, handle empty cases gracefully\n",
    "  mean_run_0 = np.mean(run_lengths_0) if run_lengths_0 else 0\n",
    "  mean_run_1 = np.mean(run_lengths_1) if run_lengths_1 else 0\n",
    "\n",
    "  return [mean_run_0, mean_run_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_weight(k):\n",
    "  #counts number of 1\n",
    "  return k.count('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_transition(segment):\n",
    "    \"\"\"\n",
    "    Count the number of bit transitions (changes from 0 to 1 or 1 to 0) in the binary string.\n",
    "\n",
    "    Parameters:\n",
    "        segment (str): Binary string.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of transitions.\n",
    "    \"\"\"\n",
    "    # Convert the binary string to a list of integers\n",
    "    data = list(map(int, segment))\n",
    "\n",
    "    # Initialize count of transitions\n",
    "    count = 0\n",
    "\n",
    "    # Loop through adjacent bits\n",
    "    for i in range(len(data) - 1):\n",
    "        count += abs(data[i] - data[i + 1])  # Compute transition\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_skewness_kurtosis(k):\n",
    "    # Convert binary string into a list of integers\n",
    "    data = list(map(int, k))\n",
    "    n = len(data)\n",
    "    # Calculate Mean\n",
    "    mean = sum(data) / n\n",
    "    # Calculate Median\n",
    "    sorted_data = sorted(data)\n",
    "    if n % 2 == 1:  # Odd number of elements\n",
    "        median = sorted_data[n // 2]\n",
    "    else:  # Even number of elements\n",
    "        median = (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2\n",
    "    # Calculate Standard Deviation\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    std_dev = variance ** 0.5\n",
    "    if std_dev == 0:  # Handle edge case where all elements are identical\n",
    "        return 0,0\n",
    "    # Calculate Skewness\n",
    "    skewness = 3 * (mean - median) / std_dev\n",
    "    # Calculate Kurtosis\n",
    "    kurtosis = sum((x - mean) ** 4 for x in data) / (n * std_dev ** 4) - 3\n",
    "\n",
    "    return skewness, kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(binary_seq, lag):\n",
    "    # Convert binary string into a list of integers\n",
    "    data = list(map(int, binary_seq))\n",
    "    n = len(data)\n",
    "    if lag >= n:\n",
    "        return \"Lag is too large for the sequence length.\"\n",
    "    # Calculate Mean\n",
    "    mean = sum(data) / n\n",
    "    # Calculate Variance\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    if variance == 0:  # Handle edge case where all elements are identical\n",
    "        return 0\n",
    "    # Calculate Autocorrelation for the given lag\n",
    "    autocorr = sum((data[t] - mean) * (data[t + lag] - mean) for t in range(n - lag)) / variance\n",
    "\n",
    "    return autocorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractal_dimension(binary_seq):\n",
    "    \"\"\"\n",
    "    Compute the fractal dimension of a binary sequence.\n",
    "\n",
    "    Parameters:\n",
    "        binary_seq (str): Binary string representing the sequence.\n",
    "\n",
    "    Returns:\n",
    "        float: Fractal dimension of the sequence.\n",
    "    \"\"\"\n",
    "    # Convert binary string into a list of integers\n",
    "    data = list(map(int, binary_seq))\n",
    "    n = len(data)\n",
    "\n",
    "    # Define possible scales\n",
    "    scales = [2**i for i in range(1, int(math.log2(n)) + 1)] if n > 1 else []\n",
    "\n",
    "    log_r = []\n",
    "    log_Nr = []\n",
    "\n",
    "    for r in scales:\n",
    "        # Divide sequence into chunks of size r\n",
    "        chunks = [data[i:i + r] for i in range(0, n, r)]\n",
    "\n",
    "        # Count non-empty chunks (chunks with at least one \"1\")\n",
    "        N_r = sum(1 for chunk in chunks if sum(chunk) > 0)\n",
    "\n",
    "        # Avoid log(0) errors\n",
    "        if N_r > 0 and r > 0:\n",
    "            log_r.append(math.log(r))\n",
    "            log_Nr.append(math.log(N_r))\n",
    "\n",
    "    # Handle edge cases where log lists are empty\n",
    "    if len(log_r) < 2 or len(log_Nr) < 2:\n",
    "        return 0.0  # Cannot compute fractal dimension for insufficient data\n",
    "\n",
    "    # Calculate slope (Fractal Dimension) using the formula: slope = Δy / Δx\n",
    "    D = (log_Nr[-1] - log_Nr[0]) / (log_r[-1] - log_r[0])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(binary_string):\n",
    "    \"\"\"\n",
    "    Extract features from binary ciphertext for training a model.\n",
    "\n",
    "    Parameters:\n",
    "        binary_string (str): Binary sequence representing the ciphertext.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing extracted features.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Feature: Length of ciphertext\n",
    "    length = len(binary_string)\n",
    "\n",
    "    # Feature: Frequency of '1's and '0's\n",
    "    ones = binary_string.count('1') / length\n",
    "    zeros = binary_string.count('0') / length\n",
    "\n",
    "    # Feature: Spectral Density (FFT mean and variance)\n",
    "    numeric_ciphertext = np.array([int(bit) for bit in binary_string]) * 2 - 1\n",
    "    fft_result = np.fft.fft(numeric_ciphertext)\n",
    "    spectral_density = np.abs(fft_result) ** 2\n",
    "    spectral_mean = np.mean(spectral_density)\n",
    "    spectral_variance = np.var(spectral_density)\n",
    "\n",
    "    # Compile features into a dictionary\n",
    "    features = [\n",
    "         spectral_mean,\n",
    "         spectral_variance\n",
    "    ]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_convolution(ciphertext, kernel=[1,-1]):\n",
    "    \"\"\"\n",
    "    Compute the convolution of a ciphertext with a given kernel.\n",
    "\n",
    "    Parameters:\n",
    "        ciphertext (list or np.array): Numeric representation of the ciphertext.\n",
    "        kernel (list or np.array): Convolutional filter or kernel.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Convolution result.\n",
    "    \"\"\"\n",
    "    # Convert ciphertext and kernel to numpy arrays for efficient computation\n",
    "    ciphertext = np.array(ciphertext)\n",
    "    kernel = np.array(kernel)\n",
    "    # Lengths of ciphertext and kernel\n",
    "    ciphertext_len = len(ciphertext)\n",
    "    kernel_len = len(kernel)\n",
    "    # Length of the output\n",
    "    output_len = ciphertext_len - kernel_len + 1\n",
    "    # Initialize the convolution result\n",
    "    convolution_result = np.zeros(output_len)\n",
    "    # Perform the convolution\n",
    "    for i in range(output_len):\n",
    "        convolution_result[i] = np.sum(ciphertext[i:i+kernel_len] * kernel)\n",
    "    return convolution_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "def compute_ciphertext_complexity(ciphertext):\n",
    "    \"\"\"\n",
    "    Compute the ciphertext complexity using zlib compression.\n",
    "\n",
    "    Parameters:\n",
    "        ciphertext (str): Ciphertext as a string.\n",
    "\n",
    "    Returns:\n",
    "        int: Length of the compressed ciphertext.\n",
    "    \"\"\"\n",
    "    # Compress the ciphertext using zlib\n",
    "    compressed_ciphertext = zlib.compress(ciphertext.encode())\n",
    "\n",
    "    # Return the length of the compressed ciphertext\n",
    "    return len(compressed_ciphertext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_features_for_row(row, segment_sizes):\n",
    "    \"\"\"\n",
    "    Extract features for a single row of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataset containing binary ciphertext in column 4.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of features extracted for the row.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    binary_string = row[4]  # Column containing binary ciphertext\n",
    "\n",
    "    # Extract features for each segment size\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]  # Binary segment (first `size` bits)\n",
    "\n",
    "        # Pad with zeros if the segment is smaller than the required size\n",
    "        segment = segment.ljust(size, '0')\n",
    "\n",
    "        # Extract features for the segment\n",
    "        features[f\"entropy_{size}\"] = entropy_analysis(segment)\n",
    "        features[f\"frequency_deviation_{size}\"] = frequency_test(segment)\n",
    "        features[f\"mean_run_0_{size}\"], features[f\"mean_run_1_{size}\"] = run_length(segment)\n",
    "        features[f\"hamming_weight_{size}\"] = hamming_weight(segment)\n",
    "        features[f\"bit_transitions_{size}\"] = bit_transition(segment)\n",
    "        features[f\"skewness_{size}\"], features[f\"kurtosis_{size}\"] = binary_skewness_kurtosis(segment)\n",
    "        features[f\"autocorrelation_{size}\"] = autocorrelation(segment, lag=4)\n",
    "        features[f\"fractal_dimension_{size}\"] = fractal_dimension(segment)\n",
    "        spectral_features = extract_features(segment)\n",
    "        features[f\"spectral_mean_{size}\"] = spectral_features[0]\n",
    "        features[f\"spectral_variance_{size}\"] = spectral_features[1]\n",
    "        convolution_result = compute_convolution([int(bit) for bit in segment])\n",
    "        features[f\"convolution_{size}\"] = sum(convolution_result)\n",
    "        features[f\"complexity_{size}\"] = compute_ciphertext_complexity(segment)\n",
    "\n",
    "    return features\n",
    "\n",
    "def append_features_to_dataset(dataset, segment_sizes=[128,256,512,1024,2048,4096], n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Append features to the dataset for each ciphertext in column 4 using parallelization.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        n_jobs (int): Number of parallel jobs (-1 means using all available processors).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with appended feature columns.\n",
    "    \"\"\"\n",
    "    # Extract features in parallel for each row\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes) for _, row in dataset.iterrows()\n",
    "    )\n",
    "\n",
    "    # Combine the features back into the DataFrame\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(data):\n",
    "    \"\"\"Perform chi-square test against uniform distribution.\"\"\"\n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    observed = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)\n",
    "    expected = np.full(256, len(data)/256)\n",
    "    chi_square = np.sum((observed - expected)**2 / expected)\n",
    "    return chi_square\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chi_square_test(data):\n",
    "    \"\"\"Perform chi-square test against uniform distribution.\"\"\"\n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    observed = np.bincount(np.frombuffer(data.encode('utf-8'), dtype=np.uint8), minlength=256)\n",
    "    expected = np.full(256, len(data) / 256)\n",
    "    chi_square = np.sum((observed - expected) ** 2 / expected)\n",
    "    return chi_square\n",
    "\n",
    "def extract_features_for_row(row, segment_sizes):\n",
    "    \"\"\"\n",
    "    Extract features for a single row of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataset containing binary ciphertext in column 4.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of features extracted for the row.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    binary_string = row[4]  # Column containing binary ciphertext\n",
    "\n",
    "    # Extract features for each segment size\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]  # Binary segment (first `size` bits)\n",
    "\n",
    "        # Pad with zeros if the segment is smaller than the required size\n",
    "        segment = segment.ljust(size, '0')\n",
    "\n",
    "        # Extract features for the segment\n",
    "        features[f\"chi_square_{size}\"] = chi_square_test(segment)\n",
    "\n",
    "    return features\n",
    "\n",
    "def append_features_to_dataset(dataset, segment_sizes=[128, 256, 512, 1024, 2048, 4096], n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Append features to the dataset for each ciphertext in column 4 using parallelization.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        n_jobs (int): Number of parallel jobs (-1 means using all available processors).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with appended feature columns.\n",
    "    \"\"\"\n",
    "    # Extract features in parallel for each row\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes) for _, row in dataset.iterrows()\n",
    "    )\n",
    "\n",
    "    # Combine the features back into the DataFrame\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_wavelet_transform(ciphertext, wavelet='haar'):\n",
    "    \"\"\"\n",
    "    Compute the discrete wavelet transform (DWT) of a binary ciphertext.\n",
    "\n",
    "    Parameters:\n",
    "        ciphertext (str): Binary ciphertext (e.g., \"10110\").\n",
    "        wavelet (str): Wavelet type (default is 'haar').\n",
    "\n",
    "    Returns:\n",
    "        list: Wavelet coefficients.\n",
    "    \"\"\"\n",
    "    # Convert binary ciphertext to numeric form (1 -> 1, 0 -> -1)\n",
    "    numeric_ciphertext = np.array([1 if bit == '1' else -1 for bit in ciphertext])\n",
    "\n",
    "    # Perform Discrete Wavelet Transform (DWT)\n",
    "    coeffs = pywt.wavedec(numeric_ciphertext, wavelet)\n",
    "\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def wavelet_transform_features(data, wavelet='haar', max_level=5):\n",
    "    \"\"\"\n",
    "    Perform wavelet transform and extract features from the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        data (str): Binary ciphertext string.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted wavelet features.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return {}\n",
    "\n",
    "    # Convert binary string to numeric values (1 for '1', -1 for '0')\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in data])\n",
    "\n",
    "    # Perform wavelet transform\n",
    "    coeffs = pywt.wavedec(numeric_data, wavelet, level=max_level)\n",
    "\n",
    "    # Extract features from each level\n",
    "    features = {}\n",
    "    for level, coeff in enumerate(coeffs):\n",
    "        features[f'wavelet_mean_level_{level}'] = np.mean(coeff)\n",
    "        features[f'wavelet_std_level_{level}'] = np.std(coeff)\n",
    "        features[f'wavelet_energy_level_{level}'] = np.sum(np.square(coeff))\n",
    "        features[f'wavelet_entropy_level_{level}'] = -np.sum(coeff * np.log2(np.abs(coeff + 1e-10)))\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_features_for_row(row, segment_sizes, wavelet='haar', max_level=4):\n",
    "    \"\"\"\n",
    "    Extract wavelet features for a single row of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataset containing binary ciphertext in column 4.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of features extracted for the row.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    binary_string = row[4]  # Column containing binary ciphertext\n",
    "\n",
    "    # Extract features for each segment size\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]  # Binary segment (first `size` bits)\n",
    "\n",
    "        # Pad with zeros if the segment is smaller than the required size\n",
    "        segment = segment.ljust(size, '0')\n",
    "\n",
    "        # Extract wavelet features for the segment\n",
    "        wavelet_features = wavelet_transform_features(segment, wavelet=wavelet, max_level=max_level)\n",
    "        features.update({f\"{key}_{size}\": value for key, value in wavelet_features.items()})\n",
    "\n",
    "    return features\n",
    "\n",
    "def append_features_to_dataset(dataset, segment_sizes=[128, 256, 512, 1024, 2048, 4096], wavelet='haar', max_level=4, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Append wavelet transform features to the dataset for each ciphertext in column 4 using parallelization.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "        n_jobs (int): Number of parallel jobs (-1 means using all available processors).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with appended feature columns.\n",
    "    \"\"\"\n",
    "    # Extract features in parallel for each row\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes, wavelet=wavelet, max_level=max_level)\n",
    "        for _, row in dataset.iterrows()\n",
    "    )\n",
    "\n",
    "    # Combine the features back into the DataFrame\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "    return updated_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def spectral_entropy(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    fft_values = np.abs(fft(numeric_data))\n",
    "    normalized_fft = fft_values / np.sum(fft_values)\n",
    "    return entropy(normalized_fft)\n",
    "\n",
    "def dominant_frequency_magnitude(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    fft_values = np.abs(fft(numeric_data))\n",
    "    return np.max(fft_values)\n",
    "\n",
    "def power_spectral_density_peaks(binary_string, num_peaks=3):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    fft_values = np.abs(fft(numeric_data)) ** 2\n",
    "    return sorted(fft_values, reverse=True)[:num_peaks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_gradient(binary_string, block_size):\n",
    "    entropies = []\n",
    "    for i in range(0, len(binary_string), block_size):\n",
    "        block = binary_string[i:i + block_size]\n",
    "        counts = np.bincount([int(bit) for bit in block], minlength=2)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropies.append(-np.sum(probabilities * np.log2(probabilities + 1e-10)))\n",
    "    return np.gradient(entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacent_bit_probability(binary_string):\n",
    "    transitions = [1 if binary_string[i] != binary_string[i + 1] else 0 for i in range(len(binary_string) - 1)]\n",
    "    return sum(transitions) / len(transitions)\n",
    "\n",
    "def bit_pair_frequency(binary_string):\n",
    "    pairs = [binary_string[i:i + 2] for i in range(len(binary_string) - 1)]\n",
    "    return {pair: pairs.count(pair) / len(pairs) for pair in ['00', '01', '10', '11']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nolds\n",
    "\n",
    "def hurst_exponent(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    return nolds.hurst_rs(numeric_data)\n",
    "\n",
    "def detrended_fluctuation_analysis(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    return nolds.dfa(numeric_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyunicorn.timeseries.recurrence_plot import RecurrencePlot\n",
    "\n",
    "def recurrence_quantification_analysis(binary_string, threshold=0.5):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    rp = RecurrencePlot(numeric_data, threshold=threshold)\n",
    "    return rp.recurrence_rate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Functions for feature extraction\n",
    "\n",
    "\n",
    "# Extract features for a single row\n",
    "def extract_features_for_row(row, segment_sizes):\n",
    "    features = {}\n",
    "    binary_string = row['4']  # Column containing binary ciphertext\n",
    "\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]\n",
    "        segment = segment.ljust(size, '0')  # Pad with zeros if smaller than size\n",
    "\n",
    "        # Compute features\n",
    "        features[f\"recurrence_rate_{size}\"] = recurrence_quantification_analysis(segment)\n",
    "        features[f\"hurst_exponent_{size}\"] = hurst_exponent(segment)\n",
    "        features[f\"dfa_{size}\"] = detrended_fluctuation_analysis(segment)\n",
    "        bit_freq = bit_pair_frequency(segment)\n",
    "        for pair, freq in bit_freq.items():\n",
    "            features[f\"bit_pair_freq_{pair}_{size}\"] = freq\n",
    "        features[f\"entropy_gradient_{size}\"] = entropy_gradient(segment)\n",
    "        features[f\"adjacent_bit_probability_{size}\"] = adjacent_bit_probability(segment)\n",
    "        features[f\"spectral_entropy_{size}\"] = spectral_entropy(segment)\n",
    "        features[f\"dominant_frequency_magnitude_{size}\"] = dominant_frequency_magnitude(segment)\n",
    "        peaks = power_spectral_density_peaks(segment)\n",
    "        for i, peak in enumerate(peaks):\n",
    "            features[f\"power_spectral_peak_{i+1}_{size}\"] = peak\n",
    "\n",
    "    return features\n",
    "\n",
    "# Append features to the dataset\n",
    "def append_features_to_dataset(dataset, segment_sizes=[128, 256, 512, 1024, 2048, 4096], n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes) for _, row in dataset.iterrows()\n",
    "    )\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "    return updated_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_str(k):\n",
    "    binary_string_final = \"\"\n",
    "    for hex_string in k:\n",
    "        pairs = [hex_string[i:i+2] for i in range(0, len(hex_string), 2)]\n",
    "        binary_list = [bin(int(pair, 16))[2:].zfill(8) for pair in pairs]\n",
    "        binary_string = ''.join(binary_list)\n",
    "        binary_string_final += binary_string\n",
    "    return binary_string_final\n",
    "\n",
    "def entropy_analysis(k):\n",
    "  #this function will be used to measure the randomness of cipher text this feature\n",
    "  # might be a little less useful\n",
    "  prob_0 = k.count('0') / len(k)\n",
    "  prob_1 = k.count('1') / len(k)\n",
    "\n",
    "    # Avoid log(0) by ensuring probabilities > 0\n",
    "  entropy = 0\n",
    "  if prob_0 > 0:\n",
    "      entropy -= prob_0 * np.log2(prob_0)\n",
    "  if prob_1 > 0:\n",
    "      entropy -= prob_1 * np.log2(prob_1)\n",
    "\n",
    "  return entropy\n",
    "\n",
    "def frequency_test(k):\n",
    "  #checks the deviation form stability\n",
    "    E_i = 0.5\n",
    "    prob_0 = k.count('0') / len(k)\n",
    "    prob_1 = k.count('1') / len(k)\n",
    "    chi_squared = (((prob_0 - E_i) ** 2) / E_i) + (((prob_1 - E_i) ** 2) / E_i)\n",
    "    deviation = chi_squared ** 0.5  # Taking the square root for stability test\n",
    "\n",
    "    return deviation\n",
    "\n",
    "def run_length(k):\n",
    "  #used to find the A run of length k\n",
    "  #consists of exactly k identical bits and\n",
    "  #is bounded before and after with a bit of opposite value.\n",
    "  runs = [(key, len(list(group))) for key, group in groupby(k)]\n",
    "\n",
    "    # Separate runs of 0s and 1s\n",
    "  run_lengths_0 = [length for bit, length in runs if bit == '0']\n",
    "  run_lengths_1 = [length for bit, length in runs if bit == '1']\n",
    "\n",
    "    # Calculate mean run lengths, handle empty cases gracefully\n",
    "  mean_run_0 = np.mean(run_lengths_0) if run_lengths_0 else 0\n",
    "  mean_run_1 = np.mean(run_lengths_1) if run_lengths_1 else 0\n",
    "\n",
    "  return [mean_run_0, mean_run_1]\n",
    "\n",
    "def hamming_weight(k):\n",
    "  #counts number of 1\n",
    "  return k.count('1')\n",
    "\n",
    "def bit_transition(segment):\n",
    "    \"\"\"\n",
    "    Count the number of bit transitions (changes from 0 to 1 or 1 to 0) in the binary string.\n",
    "\n",
    "    Parameters:\n",
    "        segment (str): Binary string.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of transitions.\n",
    "    \"\"\"\n",
    "    # Convert the binary string to a list of integers\n",
    "    data = list(map(int, segment))\n",
    "\n",
    "    # Initialize count of transitions\n",
    "    count = 0\n",
    "\n",
    "    # Loop through adjacent bits\n",
    "    for i in range(len(data) - 1):\n",
    "        count += abs(data[i] - data[i + 1])  # Compute transition\n",
    "\n",
    "    return count\n",
    "\n",
    "def binary_skewness_kurtosis(k):\n",
    "    # Convert binary string into a list of integers\n",
    "    data = list(map(int, k))\n",
    "    n = len(data)\n",
    "    # Calculate Mean\n",
    "    mean = sum(data) / n\n",
    "    # Calculate Median\n",
    "    sorted_data = sorted(data)\n",
    "    if n % 2 == 1:  # Odd number of elements\n",
    "        median = sorted_data[n // 2]\n",
    "    else:  # Even number of elements\n",
    "        median = (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2\n",
    "    # Calculate Standard Deviation\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    std_dev = variance ** 0.5\n",
    "    if std_dev == 0:  # Handle edge case where all elements are identical\n",
    "        return 0,0\n",
    "    # Calculate Skewness\n",
    "    skewness = 3 * (mean - median) / std_dev\n",
    "    # Calculate Kurtosis\n",
    "    kurtosis = sum((x - mean) ** 4 for x in data) / (n * std_dev ** 4) - 3\n",
    "\n",
    "    return skewness, kurtosis\n",
    "\n",
    "def autocorrelation(binary_seq, lag):\n",
    "    # Convert binary string into a list of integers\n",
    "    data = list(map(int, binary_seq))\n",
    "    n = len(data)\n",
    "    if lag >= n:\n",
    "        return \"Lag is too large for the sequence length.\"\n",
    "    # Calculate Mean\n",
    "    mean = sum(data) / n\n",
    "    # Calculate Variance\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    if variance == 0:  # Handle edge case where all elements are identical\n",
    "        return 0\n",
    "    # Calculate Autocorrelation for the given lag\n",
    "    autocorr = sum((data[t] - mean) * (data[t + lag] - mean) for t in range(n - lag)) / variance\n",
    "\n",
    "    return autocorr\n",
    "\n",
    "def fractal_dimension(binary_seq):\n",
    "    \"\"\"\n",
    "    Compute the fractal dimension of a binary sequence.\n",
    "\n",
    "    Parameters:\n",
    "        binary_seq (str): Binary string representing the sequence.\n",
    "\n",
    "    Returns:\n",
    "        float: Fractal dimension of the sequence.\n",
    "    \"\"\"\n",
    "    # Convert binary string into a list of integers\n",
    "    data = list(map(int, binary_seq))\n",
    "    n = len(data)\n",
    "\n",
    "    # Define possible scales\n",
    "    scales = [2**i for i in range(1, int(math.log2(n)) + 1)] if n > 1 else []\n",
    "\n",
    "    log_r = []\n",
    "    log_Nr = []\n",
    "\n",
    "    for r in scales:\n",
    "        # Divide sequence into chunks of size r\n",
    "        chunks = [data[i:i + r] for i in range(0, n, r)]\n",
    "\n",
    "        # Count non-empty chunks (chunks with at least one \"1\")\n",
    "        N_r = sum(1 for chunk in chunks if sum(chunk) > 0)\n",
    "\n",
    "        # Avoid log(0) errors\n",
    "        if N_r > 0 and r > 0:\n",
    "            log_r.append(math.log(r))\n",
    "            log_Nr.append(math.log(N_r))\n",
    "\n",
    "    # Handle edge cases where log lists are empty\n",
    "    if len(log_r) < 2 or len(log_Nr) < 2:\n",
    "        return 0.0  # Cannot compute fractal dimension for insufficient data\n",
    "\n",
    "    # Calculate slope (Fractal Dimension) using the formula: slope = Δy / Δx\n",
    "    D = (log_Nr[-1] - log_Nr[0]) / (log_r[-1] - log_r[0])\n",
    "    return D\n",
    "\n",
    "def extract_features(binary_string):\n",
    "    \"\"\"\n",
    "    Extract features from binary ciphertext for training a model.\n",
    "\n",
    "    Parameters:\n",
    "        binary_string (str): Binary sequence representing the ciphertext.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing extracted features.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Feature: Length of ciphertext\n",
    "    length = len(binary_string)\n",
    "\n",
    "    # Feature: Frequency of '1's and '0's\n",
    "    ones = binary_string.count('1') / length\n",
    "    zeros = binary_string.count('0') / length\n",
    "\n",
    "    # Feature: Spectral Density (FFT mean and variance)\n",
    "    numeric_ciphertext = np.array([int(bit) for bit in binary_string]) * 2 - 1\n",
    "    fft_result = np.fft.fft(numeric_ciphertext)\n",
    "    spectral_density = np.abs(fft_result) ** 2\n",
    "    spectral_mean = np.mean(spectral_density)\n",
    "    spectral_variance = np.var(spectral_density)\n",
    "\n",
    "    # Compile features into a dictionary\n",
    "    features = [\n",
    "         spectral_mean,\n",
    "         spectral_variance\n",
    "    ]\n",
    "\n",
    "    return features\n",
    "\n",
    "def compute_convolution(ciphertext, kernel=[1,-1]):\n",
    "    \"\"\"\n",
    "    Compute the convolution of a ciphertext with a given kernel.\n",
    "\n",
    "    Parameters:\n",
    "        ciphertext (list or np.array): Numeric representation of the ciphertext.\n",
    "        kernel (list or np.array): Convolutional filter or kernel.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Convolution result.\n",
    "    \"\"\"\n",
    "    # Convert ciphertext and kernel to numpy arrays for efficient computation\n",
    "    ciphertext = np.array(ciphertext)\n",
    "    kernel = np.array(kernel)\n",
    "    # Lengths of ciphertext and kernel\n",
    "    ciphertext_len = len(ciphertext)\n",
    "    kernel_len = len(kernel)\n",
    "    # Length of the output\n",
    "    output_len = ciphertext_len - kernel_len + 1\n",
    "    # Initialize the convolution result\n",
    "    convolution_result = np.zeros(output_len)\n",
    "    # Perform the convolution\n",
    "    for i in range(output_len):\n",
    "        convolution_result[i] = np.sum(ciphertext[i:i+kernel_len] * kernel)\n",
    "    return convolution_result\n",
    "\n",
    "import zlib\n",
    "\n",
    "def compute_ciphertext_complexity(ciphertext):\n",
    "    \"\"\"\n",
    "    Compute the ciphertext complexity using zlib compression.\n",
    "\n",
    "    Parameters:\n",
    "        ciphertext (str): Ciphertext as a string.\n",
    "\n",
    "    Returns:\n",
    "        int: Length of the compressed ciphertext.\n",
    "    \"\"\"\n",
    "    # Compress the ciphertext using zlib\n",
    "    compressed_ciphertext = zlib.compress(ciphertext.encode())\n",
    "\n",
    "    # Return the length of the compressed ciphertext\n",
    "    return len(compressed_ciphertext)\n",
    "\n",
    "def chi_square_test(segment):\n",
    "    \"\"\"\n",
    "    Perform the chi-square test on the given segment.\n",
    "    \n",
    "    Parameters:\n",
    "        segment (str): The binary string segment to test.\n",
    "\n",
    "    Returns:\n",
    "        float: The chi-square test result.\n",
    "    \"\"\"\n",
    "    # Convert the binary string into bytes (encode as ASCII)\n",
    "    data = bytes(segment, 'ascii')  # Use 'ascii' encoding for binary strings\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    \n",
    "    observed = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)\n",
    "    expected = np.full(256, len(data) / 256)\n",
    "    chi_square = np.sum((observed - expected) ** 2 / expected)\n",
    "    \n",
    "    return chi_square\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def wavelet_transform_features(data, wavelet='haar', max_level=5):\n",
    "    \"\"\"\n",
    "    Perform wavelet transform and extract features from the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        data (str): Binary ciphertext string.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted wavelet features.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return {}\n",
    "\n",
    "    # Convert binary string to numeric values (1 for '1', -1 for '0')\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in data])\n",
    "\n",
    "    # Perform wavelet transform\n",
    "    coeffs = pywt.wavedec(numeric_data, wavelet, level=max_level)\n",
    "\n",
    "    # Extract features from each level\n",
    "    features = {}\n",
    "    for level, coeff in enumerate(coeffs):\n",
    "        features[f'wavelet_mean_level_{level}'] = np.mean(coeff)\n",
    "        features[f'wavelet_std_level_{level}'] = np.std(coeff)\n",
    "        features[f'wavelet_energy_level_{level}'] = np.sum(np.square(coeff))\n",
    "        features[f'wavelet_entropy_level_{level}'] = -np.sum(coeff * np.log2(np.abs(coeff + 1e-10)))\n",
    "\n",
    "    return features\n",
    "\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def spectral_entropy(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    fft_values = np.abs(fft(numeric_data))\n",
    "    normalized_fft = fft_values / np.sum(fft_values)\n",
    "    return entropy(normalized_fft)\n",
    "\n",
    "def dominant_frequency_magnitude(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    fft_values = np.abs(fft(numeric_data))\n",
    "    return np.max(fft_values)\n",
    "\n",
    "def power_spectral_density_peaks(binary_string, num_peaks=3):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    fft_values = np.abs(fft(numeric_data)) ** 2\n",
    "    return sorted(fft_values, reverse=True)[:num_peaks]\n",
    "\n",
    "def entropy_gradient(binary_string, block_size):\n",
    "    entropies = []\n",
    "    for i in range(0, len(binary_string), block_size):\n",
    "        block = binary_string[i:i + block_size]\n",
    "        counts = np.bincount([int(bit) for bit in block], minlength=2)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropies.append(-np.sum(probabilities * np.log2(probabilities + 1e-10)))\n",
    "    return np.gradient(entropies)\n",
    "\n",
    "def adjacent_bit_probability(binary_string):\n",
    "    transitions = [1 if binary_string[i] != binary_string[i + 1] else 0 for i in range(len(binary_string) - 1)]\n",
    "    return sum(transitions) / len(transitions)\n",
    "\n",
    "def bit_pair_frequency(binary_string):\n",
    "    pairs = [binary_string[i:i + 2] for i in range(len(binary_string) - 1)]\n",
    "    return {pair: pairs.count(pair) / len(pairs) for pair in ['00', '01', '10', '11']}\n",
    "\n",
    "import nolds\n",
    "\n",
    "def hurst_exponent(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    return nolds.hurst_rs(numeric_data)\n",
    "\n",
    "def detrended_fluctuation_analysis(binary_string):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    return nolds.dfa(numeric_data)\n",
    "\n",
    "from pyunicorn.timeseries.recurrence_plot import RecurrencePlot\n",
    "\n",
    "def recurrence_quantification_analysis(binary_string, threshold=0.5):\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in binary_string])\n",
    "    rp = RecurrencePlot(numeric_data, threshold=threshold)\n",
    "    return rp.recurrence_rate()\n",
    "\n",
    "def wavelet_transform_features(data, wavelet='haar', max_level=5):\n",
    "    \"\"\"\n",
    "    Perform wavelet transform and extract features from the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        data (str): Binary ciphertext string.\n",
    "        wavelet (str): Type of wavelet to use for the transform.\n",
    "        max_level (int): Maximum level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted wavelet features.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return {}\n",
    "\n",
    "    # Convert binary string to numeric values (1 for '1', -1 for '0')\n",
    "    numeric_data = np.array([1 if bit == '1' else -1 for bit in data])\n",
    "\n",
    "    # Perform wavelet transform\n",
    "    coeffs = pywt.wavedec(numeric_data, wavelet, level=max_level)\n",
    "\n",
    "    # Extract features from each level\n",
    "    features = {}\n",
    "    for level, coeff in enumerate(coeffs):\n",
    "        features[f'wavelet_mean_level_{level}'] = np.mean(coeff)\n",
    "        features[f'wavelet_std_level_{level}'] = np.std(coeff)\n",
    "        features[f'wavelet_energy_level_{level}'] = np.sum(np.square(coeff))\n",
    "        features[f'wavelet_entropy_level_{level}'] = -np.sum(coeff * np.log2(np.abs(coeff + 1e-10)))\n",
    "\n",
    "    return features\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_features_for_row(row, segment_sizes):\n",
    "    \"\"\"\n",
    "    Extract features for a single row of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataset containing binary ciphertext in column 4.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of features extracted for the row.\n",
    "    \"\"\"\n",
    "    features = {}  # Initialize as a dictionary\n",
    "    binary_string = row[4]  # Column containing binary ciphertext\n",
    "\n",
    "    # Extract features for each segment size\n",
    "    for size in segment_sizes:\n",
    "        # Extract the segment of the required size\n",
    "        segment = binary_string[:size]  # Binary segment (first size bits)\n",
    "\n",
    "        # Pad with zeros if the segment is smaller than the required size\n",
    "        segment = segment.ljust(size, '0')\n",
    "\n",
    "        # Extract features for the segment\n",
    "        features[f\"entropy_{size}\"] = entropy_analysis(segment)\n",
    "        features[f\"frequency_deviation_{size}\"] = frequency_test(segment)\n",
    "        features[f\"mean_run_0_{size}\"], features[f\"mean_run_1_{size}\"] = run_length(segment)\n",
    "        features[f\"hamming_weight_{size}\"] = hamming_weight(segment)\n",
    "        features[f\"bit_transitions_{size}\"] = bit_transition(segment)\n",
    "        features[f\"skewness_{size}\"], features[f\"kurtosis_{size}\"] = binary_skewness_kurtosis(segment)\n",
    "        features[f\"autocorrelation_{size}\"] = autocorrelation(segment, lag=4)\n",
    "        features[f\"fractal_dimension_{size}\"] = fractal_dimension(segment)\n",
    "        spectral_features = extract_features(segment)\n",
    "        features[f\"spectral_mean_{size}\"] = spectral_features[0]\n",
    "        features[f\"spectral_variance_{size}\"] = spectral_features[1]\n",
    "        convolution_result = compute_convolution([int(bit) for bit in segment])\n",
    "        features[f\"convolution_{size}\"] = sum(convolution_result)\n",
    "        features[f\"complexity_{size}\"] = compute_ciphertext_complexity(segment)\n",
    "        features[f\"chi_square_{size}\"] = chi_square_test(segment)  # Updated call\n",
    "        wavelet_features = wavelet_transform_features(segment)\n",
    "        for key, value in wavelet_features.items():\n",
    "            features[f\"{key}_{size}\"] = value\n",
    "        features[f\"recurrence_rate_{size}\"] = recurrence_quantification_analysis(segment)\n",
    "        features[f\"hurst_exponent_{size}\"] = hurst_exponent(segment)\n",
    "        features[f\"dfa_{size}\"] = detrended_fluctuation_analysis(segment)\n",
    "        bit_freq = bit_pair_frequency(segment)\n",
    "        for pair, freq in bit_freq.items():\n",
    "            features[f\"bit_pair_freq_{pair}_{size}\"] = freq\n",
    "        features[f\"entropy_gradient_{size}\"] = entropy_gradient(segment,4)\n",
    "        features[f\"adjacent_bit_probability_{size}\"] = adjacent_bit_probability(segment)\n",
    "        features[f\"spectral_entropy_{size}\"] = spectral_entropy(segment)\n",
    "        features[f\"dominant_frequency_magnitude_{size}\"] = dominant_frequency_magnitude(segment)\n",
    "        peaks = power_spectral_density_peaks(segment)\n",
    "        for i, peak in enumerate(peaks):\n",
    "            features[f\"power_spectral_peak_{i+1}_{size}\"] = peak\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def append_features_to_dataset(dataset, segment_sizes=[8192], n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Append features to the dataset for each ciphertext in column 4 using parallelization.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "        segment_sizes (list): List of segment sizes to extract features for.\n",
    "        n_jobs (int): Number of parallel jobs (-1 means using all available processors).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with appended feature columns.\n",
    "    \"\"\"\n",
    "    # Extract features in parallel for each row\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_for_row)(row, segment_sizes) for _, row in dataset.iterrows()\n",
    "    )\n",
    "\n",
    "    # Combine the features back into the DataFrame\n",
    "    features_df = pd.DataFrame(results)\n",
    "    updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "    return updated_dataset\n",
    "# def append_features_to_dataset(dataset, segment_sizes=[8192]):\n",
    "#     \"\"\"\n",
    "#     Append features to the dataset for each ciphertext in column 4 without parallelization.\n",
    "\n",
    "#     Parameters:\n",
    "#         dataset (pd.DataFrame): Original dataset containing binary ciphertexts.\n",
    "#         segment_sizes (list): List of segment sizes to extract features for.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Updated dataset with appended feature columns.\n",
    "#     \"\"\"\n",
    "#     # Initialize an empty list to store the features for each row\n",
    "#     features_list = []\n",
    "\n",
    "#     # Iterate through the dataset rows sequentially\n",
    "#     for _, row in dataset.iterrows():\n",
    "#         # Extract features for the current row\n",
    "#         features = extract_features_for_row(row, segment_sizes)\n",
    "        \n",
    "#         # Append the features to the list\n",
    "#         features_list.append(features)\n",
    "\n",
    "#     # Combine the extracted features into a DataFrame\n",
    "#     features_df = pd.DataFrame(features_list)\n",
    "\n",
    "#     # Concatenate the original dataset with the new features\n",
    "#     updated_dataset = pd.concat([dataset.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "#     return updated_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"encrypted copy.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cv/thg3bsp52jlbdy4ztqz6b6v40000gn/T/ipykernel_21076/2988031288.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[3] = new_df[3].apply(lambda x: x[1:])\n"
     ]
    }
   ],
   "source": [
    "new_df[3] = new_df[3].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cv/thg3bsp52jlbdy4ztqz6b6v40000gn/T/ipykernel_21076/2965927233.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[4]=new_df[3].apply(transformer_str)\n"
     ]
    }
   ],
   "source": [
    "new_df[4]=new_df[3].apply(transformer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>AES-128</td>\n",
       "      <td>1</td>\n",
       "      <td>0ac1d7274c1f90b0f386d76bb84ef47990afc2c90e70c6...</td>\n",
       "      <td>0000000000001010000011000000000100001101000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>Triple-DES</td>\n",
       "      <td>1</td>\n",
       "      <td>fb33c1f018339f01427793223992dc964bc04c064a98c8...</td>\n",
       "      <td>0000111100001011000000110000001100001100000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1   2  \\\n",
       "0  BlockCipher      AES-128   1   \n",
       "1  BlockCipher   Triple-DES   1   \n",
       "\n",
       "                                                   3  \\\n",
       "0  0ac1d7274c1f90b0f386d76bb84ef47990afc2c90e70c6...   \n",
       "1  fb33c1f018339f01427793223992dc964bc04c064a98c8...   \n",
       "\n",
       "                                                   4  \n",
       "0  0000000000001010000011000000000100001101000001...  \n",
       "1  0000111100001011000000110000001100001100000000...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n",
      "Calculating recurrence plot at fixed threshold...\n",
      "Calculating the supremum distance matrix...\n"
     ]
    }
   ],
   "source": [
    "updated_dataset=append_features_to_dataset(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m updated_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mappend_features_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 407\u001b[0m, in \u001b[0;36mappend_features_to_dataset\u001b[0;34m(dataset, segment_sizes, n_jobs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03mAppend features to the dataset for each ciphertext in column 4 using parallelization.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m    pd.DataFrame: Updated dataset with appended feature columns.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# Extract features in parallel for each row\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_features_for_row\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# Combine the features back into the DataFrame\u001b[39;00m\n\u001b[1;32m    412\u001b[0m features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "updated_dataset=append_features_to_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[4]=df[3].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = pd.concat([new_df.reset_index(drop=True), df[4]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25025.000000\n",
       "mean     19369.905814\n",
       "std       5716.635009\n",
       "min         65.000000\n",
       "25%      20993.000000\n",
       "50%      21009.000000\n",
       "75%      21057.000000\n",
       "max      23137.000000\n",
       "Name: 4, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[4].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/inviforce/Downloads/sih_dataset/encrypted_hex_full.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockCipher</th>\n",
       "      <th>AES-128</th>\n",
       "      <th>1</th>\n",
       "      <th>48f629d9375ecbf87d49d9951d550322f6ac3f5133d0d4fd148166b4e7866be9f89f9c50aad63981baff9c84778d3e50335e3e95a5aa861aa3b1d7f1c23239eb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>AES-192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2b977a2fcabcdccc5cb414ca32dde21ab67d1991f4bc4e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>AES-256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>782eb042c4b6c13d13c470d9433ac6351303661c298c45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>Triple-DES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bc55f6579d46f49cc35ba3f4c160a7239fe7d58d0bb756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>Blowfish</td>\n",
       "      <td>1.0</td>\n",
       "      <td>eefffcf34b60e8880c82a72d95c3a20dda5612c81ca859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>CAST-128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>f2e78c1c07b67ce35c653556fd50de387ef1ae47577152...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BlockCipher     AES-128    1  \\\n",
       "0  BlockCipher     AES-192  1.0   \n",
       "1  BlockCipher     AES-256  1.0   \n",
       "2  BlockCipher  Triple-DES  1.0   \n",
       "3  BlockCipher    Blowfish  1.0   \n",
       "4  BlockCipher    CAST-128  1.0   \n",
       "\n",
       "  48f629d9375ecbf87d49d9951d550322f6ac3f5133d0d4fd148166b4e7866be9f89f9c50aad63981baff9c84778d3e50335e3e95a5aa861aa3b1d7f1c23239eb  \n",
       "0  2b977a2fcabcdccc5cb414ca32dde21ab67d1991f4bc4e...                                                                                \n",
       "1  782eb042c4b6c13d13c470d9433ac6351303661c298c45...                                                                                \n",
       "2  bc55f6579d46f49cc35ba3f4c160a7239fe7d58d0bb756...                                                                                \n",
       "3  eefffcf34b60e8880c82a72d95c3a20dda5612c81ca859...                                                                                \n",
       "4  f2e78c1c07b67ce35c653556fd50de387ef1ae47577152...                                                                                "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = pd.concat([df['1'].reset_index(drop=True), df['length']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5044.654667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1724.727281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8128.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "count  65114.000000\n",
       "mean    5044.654667\n",
       "std     1724.727281\n",
       "min     2002.000000\n",
       "25%     3584.000000\n",
       "50%     5088.000000\n",
       "75%     6496.000000\n",
       "max     8128.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=set(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df[:2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>AES-128</td>\n",
       "      <td>1</td>\n",
       "      <td>0ac1d7274c1f90b0f386d76bb84ef47990afc2c90e70c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>Triple-DES</td>\n",
       "      <td>1</td>\n",
       "      <td>fb33c1f018339f01427793223992dc964bc04c064a98c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1   2  \\\n",
       "0  BlockCipher      AES-128   1   \n",
       "1  BlockCipher   Triple-DES   1   \n",
       "\n",
       "                                                   3  \n",
       "0   0ac1d7274c1f90b0f386d76bb84ef47990afc2c90e70c...  \n",
       "1   fb33c1f018339f01427793223992dc964bc04c064a98c...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[3] = [k[1:] for k in new_df[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>AES-128</td>\n",
       "      <td>1</td>\n",
       "      <td>0ac1d7274c1f90b0f386d76bb84ef47990afc2c90e70c6...</td>\n",
       "      <td>0000000000001010000011000000000100001101000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>Triple-DES</td>\n",
       "      <td>1</td>\n",
       "      <td>fb33c1f018339f01427793223992dc964bc04c064a98c8...</td>\n",
       "      <td>0000111100001011000000110000001100001100000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1   2  \\\n",
       "0  BlockCipher      AES-128   1   \n",
       "1  BlockCipher   Triple-DES   1   \n",
       "\n",
       "                                                   3  \\\n",
       "0  0ac1d7274c1f90b0f386d76bb84ef47990afc2c90e70c6...   \n",
       "1  fb33c1f018339f01427793223992dc964bc04c064a98c8...   \n",
       "\n",
       "                                                   4  \n",
       "0  0000000000001010000011000000000100001101000001...  \n",
       "1  0000111100001011000000110000001100001100000000...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[4]=new_df[3].apply(transformer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>AES-128</td>\n",
       "      <td>1</td>\n",
       "      <td>0ac1d7274c1f90b0f386d76bb84ef47990afc2c90e70c6...</td>\n",
       "      <td>0000000000001010000011000000000100001101000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlockCipher</td>\n",
       "      <td>Triple-DES</td>\n",
       "      <td>1</td>\n",
       "      <td>fb33c1f018339f01427793223992dc964bc04c064a98c8...</td>\n",
       "      <td>0000111100001011000000110000001100001100000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1   2  \\\n",
       "0  BlockCipher      AES-128   1   \n",
       "1  BlockCipher   Triple-DES   1   \n",
       "\n",
       "                                                   3  \\\n",
       "0  0ac1d7274c1f90b0f386d76bb84ef47990afc2c90e70c6...   \n",
       "1  fb33c1f018339f01427793223992dc964bc04c064a98c8...   \n",
       "\n",
       "                                                   4  \n",
       "0  0000000000001010000011000000000100001101000001...  \n",
       "1  0000111100001011000000110000001100001100000000...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
